{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anastasiarenata1/deeplearning/blob/main/LNN_with_Numpy_and_Softmax_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRQriJO6Rw4D"
      },
      "source": [
        "# Problem 4 - Regression\n",
        "\n",
        "Classification data from 2011 Million Song Challenge dataset to predict music year\n",
        "\n",
        "* Explore three shallow (linear) neural network models with different activation functions for this task.\n",
        "* Evaluate the model by rounding the output of your linear neural network and compute the mean squared error\n",
        "\n",
        "\n",
        "###1. Load and explore the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEErmxZU2EDU",
        "outputId": "367194d3-32ec-4685-8bd4-9db4114b0e95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-02 04:33:15--  https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 211011981 (201M) [application/x-httpd-php]\n",
            "Saving to: ‘YearPredictionMSD.txt.zip’\n",
            "\n",
            "YearPredictionMSD.t 100%[===================>] 201.24M  17.3MB/s    in 13s     \n",
            "\n",
            "2023-03-02 04:33:29 (15.2 MB/s) - ‘YearPredictionMSD.txt.zip’ saved [211011981/211011981]\n",
            "\n",
            "Archive:  YearPredictionMSD.txt.zip\n",
            "  inflating: YearPredictionMSD.txt   \n"
          ]
        }
      ],
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n",
        "!unzip YearPredictionMSD.txt.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jYGlyolcU0ai"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "BbPalayaR_Kz",
        "outputId": "f27f0be8-78a1-44b9-905b-e7cec4fbe236"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   target  timbre_avg_1  timbre_avg_2  timbre_avg_3  timbre_avg_4  \\\n",
              "0    2001      49.94357      21.47114      73.07750       8.74861   \n",
              "1    2001      48.73215      18.42930      70.32679      12.94636   \n",
              "2    2001      50.95714      31.85602      55.81851      13.41693   \n",
              "3    2001      48.24750      -1.89837      36.29772       2.58776   \n",
              "4    2001      50.97020      42.20998      67.09964       8.46791   \n",
              "\n",
              "   timbre_avg_5  timbre_avg_6  timbre_avg_7  timbre_avg_8  timbre_avg_9  ...  \\\n",
              "0     -17.40628     -13.09905     -25.01202     -12.23257       7.83089  ...   \n",
              "1     -10.32437     -24.83777       8.76630      -0.92019      18.76548  ...   \n",
              "2      -6.57898     -18.54940      -3.27872      -2.35035      16.07017  ...   \n",
              "3       0.97170     -26.21683       5.05097     -10.34124       3.55005  ...   \n",
              "4     -15.85279     -16.81409     -12.48207      -9.37636      12.63699  ...   \n",
              "\n",
              "   timbre_covar_69  timbre_covar_70  timbre_covar_71  timbre_covar_72  \\\n",
              "0         13.01620        -54.40548         58.99367         15.37344   \n",
              "1          5.66812        -19.68073         33.04964         42.87836   \n",
              "2          3.03800         26.05866        -50.92779         10.93792   \n",
              "3         34.57337       -171.70734        -16.96705        -46.67617   \n",
              "4          9.92661        -55.95724         64.92712        -17.72522   \n",
              "\n",
              "   timbre_covar_73  timbre_covar_74  timbre_covar_75  timbre_covar_76  \\\n",
              "0          1.11144        -23.08793         68.40795         -1.82223   \n",
              "1         -9.90378        -32.22788         70.49388         12.04941   \n",
              "2         -0.07568         43.20130       -115.00698         -0.05859   \n",
              "3        -12.51516         82.58061        -72.08993          9.90558   \n",
              "4         -1.49237         -7.50035         51.76631          7.88713   \n",
              "\n",
              "   timbre_covar_77  timbre_covar_78  \n",
              "0        -27.46348          2.26327  \n",
              "1         58.43453         26.92061  \n",
              "2         39.67068         -0.66345  \n",
              "3        199.62971         18.85382  \n",
              "4         55.66926         28.74903  \n",
              "\n",
              "[5 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-940276d3-d9c0-40c8-820f-1b3f8904da41\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>timbre_avg_1</th>\n",
              "      <th>timbre_avg_2</th>\n",
              "      <th>timbre_avg_3</th>\n",
              "      <th>timbre_avg_4</th>\n",
              "      <th>timbre_avg_5</th>\n",
              "      <th>timbre_avg_6</th>\n",
              "      <th>timbre_avg_7</th>\n",
              "      <th>timbre_avg_8</th>\n",
              "      <th>timbre_avg_9</th>\n",
              "      <th>...</th>\n",
              "      <th>timbre_covar_69</th>\n",
              "      <th>timbre_covar_70</th>\n",
              "      <th>timbre_covar_71</th>\n",
              "      <th>timbre_covar_72</th>\n",
              "      <th>timbre_covar_73</th>\n",
              "      <th>timbre_covar_74</th>\n",
              "      <th>timbre_covar_75</th>\n",
              "      <th>timbre_covar_76</th>\n",
              "      <th>timbre_covar_77</th>\n",
              "      <th>timbre_covar_78</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 91 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-940276d3-d9c0-40c8-820f-1b3f8904da41')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-940276d3-d9c0-40c8-820f-1b3f8904da41 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-940276d3-d9c0-40c8-820f-1b3f8904da41');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "colnames = ['target'] + ['timbre_avg_' + str(i) for i in range(1, 13)] + ['timbre_covar_' + str(i) for i in range(1, 79)]\n",
        "df = pd.read_csv('YearPredictionMSD.txt', header=None, names=colnames)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqj7q70Ql5lb"
      },
      "source": [
        "Write a function to load the dataset, e.g.,\n",
        "`trainYears, trainFeat, testYears, testFeat = loadMusicData(fname, addBias)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HO0rnbz9STFd"
      },
      "outputs": [],
      "source": [
        "def loadMusicData(data, addBias=True):\n",
        "  train_df = data[:463714]\n",
        "  test_df = data[463714:]\n",
        "  train_y = train_df['target'].values\n",
        "  train_x = train_df.iloc[:,1:].values\n",
        "  test_y = test_df['target'].values\n",
        "  test_x = test_df.iloc[:,1:].values\n",
        "  if addBias:\n",
        "    train_x = np.hstack((train_x, np.ones((train_x.shape[0],1))))\n",
        "    test_x = np.hstack((test_x, np.ones((test_x.shape[0],1))))\n",
        "  return train_y, train_x, test_y, test_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz0sr2dbmDpu"
      },
      "source": [
        "Write a function `mse = musicMSE(pred, gt)` where the inputs are the predicted year and the “ground truth” year from the dataset. The function computes the mean squared error(MSE) by rounding pred before computing the MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eBi_cfQ4nGmY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "def musicMSE(pred, gt):\n",
        "  pred = np.round(pred)\n",
        "  mse= mean_squared_error(pred, gt)\n",
        "  return mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNpBvB08ojLl"
      },
      "source": [
        "Load the dataset and discuss its properties. \n",
        "1. What is the range of the variables? From 90 attributes, range of variables for timbre average is tighter than range of variables for timbre covariance. However, within each category itself, some attributes have wider range compared to others, in which we don't have further documentation to explain this event.\n",
        "2. How might you normalize them? Normalization can help to ensure that each variables contribute equally to the model. Since range of variables varies significantly accross 90 attributes, I will normalize the data using standardization technique (0 mean and unit std deviation for each attribute) where we can help to preserve importance of variables.\n",
        "3. What years are represented in the dataset? The dataset covers song released from 1922 to 2011 (90 years) with most common year of 2007.\n",
        "4. What will the test mean squared error (MSE) be if your classifier always outputs the most common year in the dataset? 190.08"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwHVPkNVoTmn",
        "outputId": "97173a41-b477-4bc0-c63f-c8823ab129b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variable 1: range = 60\n",
            "Variable 2: range = 721\n",
            "Variable 3: range = 624\n",
            "Variable 4: range = 490\n",
            "Variable 5: range = 444\n",
            "Variable 6: range = 248\n",
            "Variable 7: range = 361\n",
            "Variable 8: range = 199\n",
            "Variable 9: range = 273\n",
            "Variable 10: range = 102\n",
            "Variable 11: range = 158\n",
            "Variable 12: range = 182\n",
            "Variable 13: range = 550\n",
            "Variable 14: range = 65727\n",
            "Variable 15: range = 36796\n",
            "Variable 16: range = 31832\n",
            "Variable 17: range = 19854\n",
            "Variable 18: range = 16826\n",
            "Variable 19: range = 11882\n",
            "Variable 20: range = 9564\n",
            "Variable 21: range = 9610\n",
            "Variable 22: range = 3707\n",
            "Variable 23: range = 6731\n",
            "Variable 24: range = 9808\n",
            "Variable 25: range = 4871\n",
            "Variable 26: range = 37870\n",
            "Variable 27: range = 26522\n",
            "Variable 28: range = 7735\n",
            "Variable 29: range = 6635\n",
            "Variable 30: range = 6669\n",
            "Variable 31: range = 6153\n",
            "Variable 32: range = 3471\n",
            "Variable 33: range = 4567\n",
            "Variable 34: range = 3921\n",
            "Variable 35: range = 2803\n",
            "Variable 36: range = 4208\n",
            "Variable 37: range = 22597\n",
            "Variable 38: range = 18155\n",
            "Variable 39: range = 15869\n",
            "Variable 40: range = 16243\n",
            "Variable 41: range = 8211\n",
            "Variable 42: range = 8068\n",
            "Variable 43: range = 6178\n",
            "Variable 44: range = 2904\n",
            "Variable 45: range = 1768\n",
            "Variable 46: range = 2529\n",
            "Variable 47: range = 23921\n",
            "Variable 48: range = 10960\n",
            "Variable 49: range = 12815\n",
            "Variable 50: range = 4412\n",
            "Variable 51: range = 3698\n",
            "Variable 52: range = 4207\n",
            "Variable 53: range = 5583\n",
            "Variable 54: range = 5100\n",
            "Variable 55: range = 2263\n",
            "Variable 56: range = 12109\n",
            "Variable 57: range = 17812\n",
            "Variable 58: range = 17732\n",
            "Variable 59: range = 9176\n",
            "Variable 60: range = 8190\n",
            "Variable 61: range = 3711\n",
            "Variable 62: range = 2370\n",
            "Variable 63: range = 1413\n",
            "Variable 64: range = 21394\n",
            "Variable 65: range = 10254\n",
            "Variable 66: range = 7344\n",
            "Variable 67: range = 3254\n",
            "Variable 68: range = 7345\n",
            "Variable 69: range = 7192\n",
            "Variable 70: range = 1720\n",
            "Variable 71: range = 11016\n",
            "Variable 72: range = 11695\n",
            "Variable 73: range = 10525\n",
            "Variable 74: range = 3453\n",
            "Variable 75: range = 2666\n",
            "Variable 76: range = 1459\n",
            "Variable 77: range = 19852\n",
            "Variable 78: range = 5449\n",
            "Variable 79: range = 13578\n",
            "Variable 80: range = 8490\n",
            "Variable 81: range = 1279\n",
            "Variable 82: range = 8872\n",
            "Variable 83: range = 5021\n",
            "Variable 84: range = 4832\n",
            "Variable 85: range = 602\n",
            "Variable 86: range = 6831\n",
            "Variable 87: range = 7154\n",
            "Variable 88: range = 699\n",
            "Variable 89: range = 14852\n",
            "Variable 90: range = 1059\n"
          ]
        }
      ],
      "source": [
        "# Range of variables\n",
        "var_ranges = np.ptp(df.iloc[:, 1:].values, axis=0)\n",
        "for i, var_range in enumerate(var_ranges):\n",
        "    print(\"Variable {}: range = {:.0f}\".format(i+1, var_range))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6xcZrqpiyO0F"
      },
      "outputs": [],
      "source": [
        "#Normalize data using standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "def loadMusicData2(data, addBias=True):\n",
        "  train_df = data[:463714]\n",
        "  test_df = data[463714:]\n",
        "  train_y = train_df['target'].values\n",
        "  train_x = train_df.iloc[:,1:].values\n",
        "  test_y = test_df['target'].values\n",
        "  test_x = test_df.iloc[:,1:].values\n",
        "  train_x= scaler.fit_transform(train_x)\n",
        "  test_x= scaler.fit_transform(test_x)\n",
        "\n",
        "  if addBias:\n",
        "    train_x = np.hstack((train_x, np.ones((train_x.shape[0],1))))\n",
        "    test_x = np.hstack((test_x, np.ones((test_x.shape[0],1))))\n",
        "  \n",
        "\n",
        "  return train_y, train_x, test_y, test_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J88tbEW8y56c",
        "outputId": "03f341f4-ea2e-455a-98fa-95e1436a1f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min years presented:  1922\n",
            "Max years presented:  2011\n",
            "Median years presented:  2002.0\n",
            "Most common year: 2007\n"
          ]
        }
      ],
      "source": [
        "#Years represented\n",
        "print('Min years presented: ', np.min(df['target']))\n",
        "print('Max years presented: ', np.max(df['target']))\n",
        "print('Median years presented: ', np.median(df['target']))\n",
        "\n",
        "from statistics import mode\n",
        "print('Most common year:', mode(df['target']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "XsuMY-B-6Rq8"
      },
      "outputs": [],
      "source": [
        "train_y, train_x, test_y, test_x = loadMusicData2(df, addBias=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgxWh8BG6E15",
        "outputId": "7461d868-e5cd-4052-f6bf-ccdaab1b4d10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "190.08239236117836"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "#test MSE\n",
        "musicMSE(torch.full((test_y.shape[0],), 2007), test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzi847OP89JV"
      },
      "source": [
        "##2. Classification\n",
        "This problem could have been posed as a classification problem by treating each year as a category. What would be the problems with this approach? Support your argument by analyzing a bar chart with the year as the x-axis and the number of examples for that year as the y-axis.\n",
        "\n",
        "As we can see from the chart, the distribution of train dataset is skewed to the left where majority of the data coming from the later years. If we treat this problem as a classification problem, the model will be biased and will be more likely to predict later years. Furthermore, classification means that the predicted data will be categorical values instead of continuous which can result in loss of information as repercussion. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "QByIY0eo3_uO",
        "outputId": "8ccfabf9-9dca-4e88-b3e5-1bb6f2753a2b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#plot year frequency from train dataset\n",
        "import matplotlib.pyplot as plt\n",
        "train_df = df[:463714]\n",
        "year_counts = train_df.iloc[:, 0].value_counts()\n",
        "plt.bar(year_counts.index, year_counts.values)\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiR5yP7wBxka"
      },
      "source": [
        "##3. Ridge regression\n",
        "\n",
        "* Implement stochastic gradient descent with mini-batches to minimize the loss and evaluate the train and test MSE.\n",
        "* Tune the learning rate and weight decay factor. \n",
        "* Show the train and test loss as a function of epochs, where the number of epochs should be chosen to ensure the train loss is minimized."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_batch_gradient_descent(X_train, y_train, X_test, y_test, batch_size=32, num_epochs=100, learning_rate=0.01, weight_decay_factor=0, loss_type='L2', weight_decay_form='none', momentum=False, momentum_factor=0.9):\n",
        "    num_features = X_train.shape[1]\n",
        "    num_batches = int(np.ceil(len(X_train) / batch_size))\n",
        "    weight = np.random.normal(size=num_features)\n",
        "    m = np.zeros_like(weight)\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    def forward(X, w):\n",
        "        return np.dot(X, w)\n",
        "\n",
        "    def backward(X, error):\n",
        "        return np.dot(X.T, error)\n",
        "\n",
        "    def compute_gradient(X, y, y_pred, loss_type, w):\n",
        "      error = None\n",
        "      if loss_type == \"L2\":\n",
        "          error = 2*(y_pred - y)\n",
        "      elif loss_type == \"count\":\n",
        "          error = np.round(y_pred) - np.round(y)\n",
        "      elif loss_type == \"cross-entropy\":\n",
        "          error = y_pred - y\n",
        "      elif loss_type == 'L1':\n",
        "          error = np.sign(y_pred - y)\n",
        "      gradient = backward(X, error)\n",
        "      if weight_decay_form == 'L2':\n",
        "          gradient += weight_decay_factor * w\n",
        "      elif weight_decay_form == 'L1':\n",
        "          gradient += weight_decay_factor * np.sign(w)\n",
        "      return gradient, error\n",
        "\n",
        "    def compute_loss(y, y_pred, loss_type):\n",
        "        if loss_type == \"L2\":\n",
        "            return np.mean(np.square(y - y_pred))\n",
        "        elif loss_type == \"count\":\n",
        "            return np.mean(np.abs(np.round(y) - np.round(y_pred)))\n",
        "        elif loss_type == \"cross-entropy\":\n",
        "            y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # clip predictions\n",
        "            return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "        elif loss_type == 'L1':\n",
        "            return np.mean(np.abs(y - y_pred))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle the data\n",
        "        perm = np.random.permutation(len(X_train))\n",
        "        X_train = X_train[perm]\n",
        "        y_train = y_train[perm]\n",
        "\n",
        "        # Mini-batch gradient descent\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = (i + 1) * batch_size\n",
        "            X_batch = X_train[start_idx:end_idx]\n",
        "            y_batch = y_train[start_idx:end_idx]\n",
        "\n",
        "            y_pred = forward(X_batch, weight)\n",
        "            gradient, error = compute_gradient(X_batch, y_batch, y_pred, loss_type, weight)\n",
        "\n",
        "            if momentum:\n",
        "              m = momentum_factor * m + (1 - momentum_factor) * gradient\n",
        "              weight -= learning_rate * m\n",
        "            else:\n",
        "                weight -= learning_rate * gradient.reshape(weight.shape)\n",
        "\n",
        "        # Compute train and test losses\n",
        "        y_train_pred = forward(X_train, weight)\n",
        "        train_loss = compute_loss(y_train, y_train_pred, loss_type)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        y_test_pred = forward(X_test, weight)\n",
        "        test_loss = compute_loss(y_test, y_test_pred, loss_type)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        print(\"Epoch:\", epoch+1, \"/100, Train loss:\", train_loss, \"Test loss:\", test_loss)\n",
        "\n",
        "    return weight, train_losses, test_losses\n"
      ],
      "metadata": {
        "id": "f7u12GIuw_jz"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning\n",
        "* Learning rate= 0.0001\n",
        "* Weight decay factor= 0.000001\n",
        "* Batch size=16\n",
        "* Number epochs=100\n",
        "* Loss type= L2\n",
        "* Weight decay form= None\n",
        "* No momentum\n",
        "\n"
      ],
      "metadata": {
        "id": "r27YeMIDlVMX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "lHLX_tNcbLWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3540d4e1-8efd-4324-d8ea-e648dab2d224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 /100, Train loss: 625031.7119610738 Test loss: 625203.0811098132\n",
            "Epoch: 2 /100, Train loss: 97870.4163194665 Test loss: 97937.24304554866\n",
            "Epoch: 3 /100, Train loss: 15392.919365339705 Test loss: 15419.069182976817\n",
            "Epoch: 4 /100, Train loss: 2488.3059055079752 Test loss: 2498.031320338508\n",
            "Epoch: 5 /100, Train loss: 468.1861523742782 Test loss: 471.5198856097959\n",
            "Epoch: 6 /100, Train loss: 151.67055066751848 Test loss: 152.47556917553456\n",
            "Epoch: 7 /100, Train loss: 101.75609316415725 Test loss: 101.54875612326255\n",
            "Epoch: 8 /100, Train loss: 93.69772447775267 Test loss: 93.07710232353566\n",
            "Epoch: 9 /100, Train loss: 92.25594079939825 Test loss: 91.53873515886673\n",
            "Epoch: 10 /100, Train loss: 91.904368521772 Test loss: 91.13747164799591\n",
            "Epoch: 11 /100, Train loss: 91.75992685818932 Test loss: 90.99228741314393\n",
            "Epoch: 12 /100, Train loss: 91.65899165104233 Test loss: 90.84925917027796\n",
            "Epoch: 13 /100, Train loss: 91.587272858718 Test loss: 90.77708813294805\n",
            "Epoch: 14 /100, Train loss: 91.53491094547635 Test loss: 90.76774287519638\n",
            "Epoch: 15 /100, Train loss: 91.49109434447112 Test loss: 90.71948664822747\n",
            "Epoch: 16 /100, Train loss: 91.45705521369666 Test loss: 90.68598664211247\n",
            "Epoch: 17 /100, Train loss: 91.4314776513457 Test loss: 90.65415332606776\n",
            "Epoch: 18 /100, Train loss: 91.40523305030477 Test loss: 90.6325302856026\n",
            "Epoch: 19 /100, Train loss: 91.38643800418626 Test loss: 90.59583266852721\n",
            "Epoch: 20 /100, Train loss: 91.3694432681569 Test loss: 90.57901060495202\n",
            "Epoch: 21 /100, Train loss: 91.36033113857704 Test loss: 90.60336321108142\n",
            "Epoch: 22 /100, Train loss: 91.34715197666846 Test loss: 90.56228538350577\n",
            "Epoch: 23 /100, Train loss: 91.33323432852104 Test loss: 90.56220374461752\n",
            "Epoch: 24 /100, Train loss: 91.32637988686594 Test loss: 90.53954935200032\n",
            "Epoch: 25 /100, Train loss: 91.32065095469747 Test loss: 90.5586020863008\n",
            "Epoch: 26 /100, Train loss: 91.30937907041117 Test loss: 90.53392982966587\n",
            "Epoch: 27 /100, Train loss: 91.30505062659374 Test loss: 90.55253631763048\n",
            "Epoch: 28 /100, Train loss: 91.29906906921951 Test loss: 90.53115186990381\n",
            "Epoch: 29 /100, Train loss: 91.29476076884673 Test loss: 90.50117609149656\n",
            "Epoch: 30 /100, Train loss: 91.29368795517298 Test loss: 90.55068747508831\n",
            "Epoch: 31 /100, Train loss: 91.2879352002343 Test loss: 90.51513581283321\n",
            "Epoch: 32 /100, Train loss: 91.28367505554857 Test loss: 90.52923926126795\n",
            "Epoch: 33 /100, Train loss: 91.28070117977047 Test loss: 90.53304536543264\n",
            "Epoch: 34 /100, Train loss: 91.27873535387324 Test loss: 90.53490502274627\n",
            "Epoch: 35 /100, Train loss: 91.27670522737402 Test loss: 90.49807086187666\n",
            "Epoch: 36 /100, Train loss: 91.28160023813967 Test loss: 90.4884966398778\n",
            "Epoch: 37 /100, Train loss: 91.27450150838544 Test loss: 90.47770925244797\n",
            "Epoch: 38 /100, Train loss: 91.27024377947646 Test loss: 90.51716300971997\n",
            "Epoch: 39 /100, Train loss: 91.27074609624607 Test loss: 90.50428281907725\n",
            "Epoch: 40 /100, Train loss: 91.26822342877546 Test loss: 90.52300804436938\n",
            "Epoch: 41 /100, Train loss: 91.26745762834386 Test loss: 90.49506762749292\n",
            "Epoch: 42 /100, Train loss: 91.26935343812032 Test loss: 90.49515825393136\n",
            "Epoch: 43 /100, Train loss: 91.26901745586241 Test loss: 90.53512482414571\n",
            "Epoch: 44 /100, Train loss: 91.26425022808546 Test loss: 90.50679918360365\n",
            "Epoch: 45 /100, Train loss: 91.26566791761687 Test loss: 90.4777513579489\n",
            "Epoch: 46 /100, Train loss: 91.26635597418534 Test loss: 90.47848259749803\n",
            "Epoch: 47 /100, Train loss: 91.26248193960977 Test loss: 90.49215698365167\n",
            "Epoch: 48 /100, Train loss: 91.26277330408372 Test loss: 90.49900815828029\n",
            "Epoch: 49 /100, Train loss: 91.26965510534774 Test loss: 90.49955481792968\n",
            "Epoch: 50 /100, Train loss: 91.26276890177222 Test loss: 90.49462241760332\n",
            "Epoch: 51 /100, Train loss: 91.26263139942274 Test loss: 90.52553062118535\n",
            "Epoch: 52 /100, Train loss: 91.26392146123818 Test loss: 90.46962231676137\n",
            "Epoch: 53 /100, Train loss: 91.26117959639983 Test loss: 90.48699261083681\n",
            "Epoch: 54 /100, Train loss: 91.26089271207742 Test loss: 90.49075790424453\n",
            "Epoch: 55 /100, Train loss: 91.25978420305573 Test loss: 90.48608062470247\n",
            "Epoch: 56 /100, Train loss: 91.26124812242384 Test loss: 90.4755493836881\n",
            "Epoch: 57 /100, Train loss: 91.26020709036327 Test loss: 90.51958890626993\n",
            "Epoch: 58 /100, Train loss: 91.26050048901794 Test loss: 90.4726527033059\n",
            "Epoch: 59 /100, Train loss: 91.26074129119127 Test loss: 90.50693071923135\n",
            "Epoch: 60 /100, Train loss: 91.25945145959183 Test loss: 90.49211316693129\n",
            "Epoch: 61 /100, Train loss: 91.26236256511808 Test loss: 90.50560756494751\n",
            "Epoch: 62 /100, Train loss: 91.26468457503626 Test loss: 90.53853887660175\n",
            "Epoch: 63 /100, Train loss: 91.26068651811279 Test loss: 90.51770094162812\n",
            "Epoch: 64 /100, Train loss: 91.25951990722412 Test loss: 90.47693786895833\n",
            "Epoch: 65 /100, Train loss: 91.25919463355027 Test loss: 90.51292551035347\n",
            "Epoch: 66 /100, Train loss: 91.26150232913429 Test loss: 90.50822961620374\n",
            "Epoch: 67 /100, Train loss: 91.259499970124 Test loss: 90.5005530556394\n",
            "Epoch: 68 /100, Train loss: 91.26095754474566 Test loss: 90.4812065042295\n",
            "Epoch: 69 /100, Train loss: 91.26278827343208 Test loss: 90.45502946360165\n",
            "Epoch: 70 /100, Train loss: 91.25980160374336 Test loss: 90.48967477738701\n",
            "Epoch: 71 /100, Train loss: 91.26184673887867 Test loss: 90.45781747989132\n",
            "Epoch: 72 /100, Train loss: 91.26106063244927 Test loss: 90.45404033023662\n",
            "Epoch: 73 /100, Train loss: 91.25857281065085 Test loss: 90.4856444381148\n",
            "Epoch: 74 /100, Train loss: 91.26115395014031 Test loss: 90.46557545062845\n",
            "Epoch: 75 /100, Train loss: 91.25935607743695 Test loss: 90.47591188806278\n",
            "Epoch: 76 /100, Train loss: 91.25825426619114 Test loss: 90.47324296304947\n",
            "Epoch: 77 /100, Train loss: 91.26236784212125 Test loss: 90.49021870045225\n",
            "Epoch: 78 /100, Train loss: 91.25873822447218 Test loss: 90.47964656878361\n",
            "Epoch: 79 /100, Train loss: 91.25771339819154 Test loss: 90.47395090188479\n",
            "Epoch: 80 /100, Train loss: 91.2629354245333 Test loss: 90.53846268975525\n",
            "Epoch: 81 /100, Train loss: 91.25911266997213 Test loss: 90.4717444573685\n",
            "Epoch: 82 /100, Train loss: 91.25981475364355 Test loss: 90.48000534391998\n",
            "Epoch: 83 /100, Train loss: 91.26094414880734 Test loss: 90.48610387455577\n",
            "Epoch: 84 /100, Train loss: 91.26083567552027 Test loss: 90.5145315472638\n",
            "Epoch: 85 /100, Train loss: 91.25896243324318 Test loss: 90.48315225073219\n",
            "Epoch: 86 /100, Train loss: 91.25803527264364 Test loss: 90.49824638617311\n",
            "Epoch: 87 /100, Train loss: 91.25797132715199 Test loss: 90.50883755258901\n",
            "Epoch: 88 /100, Train loss: 91.25767876940463 Test loss: 90.4918248932297\n",
            "Epoch: 89 /100, Train loss: 91.25738594183484 Test loss: 90.49385406801365\n",
            "Epoch: 90 /100, Train loss: 91.2581614162357 Test loss: 90.47502509020303\n",
            "Epoch: 91 /100, Train loss: 91.2576675545734 Test loss: 90.49337464232528\n",
            "Epoch: 92 /100, Train loss: 91.26092241195408 Test loss: 90.47585891148893\n",
            "Epoch: 93 /100, Train loss: 91.2574600086544 Test loss: 90.50232810282064\n",
            "Epoch: 94 /100, Train loss: 91.26022354981447 Test loss: 90.50888907541474\n",
            "Epoch: 95 /100, Train loss: 91.25762237453132 Test loss: 90.49682666551915\n",
            "Epoch: 96 /100, Train loss: 91.26130302765175 Test loss: 90.49894413471215\n",
            "Epoch: 97 /100, Train loss: 91.25814041502124 Test loss: 90.4745709604753\n",
            "Epoch: 98 /100, Train loss: 91.2642262948356 Test loss: 90.52619186176717\n",
            "Epoch: 99 /100, Train loss: 91.26031784097245 Test loss: 90.49445701034179\n",
            "Epoch: 100 /100, Train loss: 91.26426391445224 Test loss: 90.47197015231247\n"
          ]
        }
      ],
      "source": [
        "w_ridge, train_loss_ridge, test_loss_ridge = mini_batch_gradient_descent(train_x, train_y, test_x, test_y, batch_size=16, num_epochs=100, learning_rate=0.000001, weight_decay_factor=0.000001, loss_type='L2', weight_decay_form=None, momentum=False, momentum_factor=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "a7olCs7GbNW5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "93c9a152-1f66-45b1-9851-186869fbcc37"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "num_epochs=100\n",
        "plt.plot(range(num_epochs), train_loss_ridge,'r', label=\"Train loss\")\n",
        "plt.plot(range(num_epochs), test_loss_ridge, 'bo',label=\"Test loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9ap9a_o1OGE"
      },
      "source": [
        "Pseudoinverse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(X, y, weight, loss_type):\n",
        "    if loss_type == 'L2':\n",
        "        loss = np.mean((np.dot(X, weight) - y) ** 2)\n",
        "    elif loss_type == 'count':\n",
        "        loss = np.mean(np.abs(np.dot(X, weight) - y))\n",
        "    elif loss_type == 'cross-entropy':\n",
        "        exp_term = np.exp(np.dot(X, weight))\n",
        "        loss = np.mean(np.log(1 + exp_term) - y * np.dot(X, weight))\n",
        "    return loss\n",
        "\n",
        "def pseudoinverse(train_x, train_y, test_x, test_y, alpha=0, loss_type='L2'):\n",
        "    pseudoinv= np.dot(np.linalg.inv(train_x.T.dot(train_x) + alpha*np.eye(train_x.shape[1])), train_x.T)\n",
        "    weight= np.dot(pseudoinv, train_y)\n",
        "    train_y_predict= np.dot(train_x, weight)\n",
        "    test_y_predict= np.dot(test_x, weight)\n",
        "\n",
        "    train_loss= compute_loss(train_x, train_y, weight, loss_type=loss_type)\n",
        "    test_loss= compute_loss(test_x, test_y,weight, loss_type=loss_type)\n",
        "  \n",
        "    print(f'Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}')\n",
        "    \n",
        "    return weight, train_loss, test_loss"
      ],
      "metadata": {
        "id": "ohY45Ph3pYrd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_inverse, train_loss_inverse, test_loss_inverse = pseudoinverse(train_x, train_y, test_x, test_y, alpha=0, loss_type='L2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUVECvJdDX7I",
        "outputId": "f514d1e1-89e5-4ae5-c1a2-b447afc5b99e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 91.2564, Test loss: 90.4911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Implement L1 weight decay"
      ],
      "metadata": {
        "id": "jL8oRaDeDY-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_lasso, train_loss_lasso, test_loss_lasso = mini_batch_gradient_descent(train_x, train_y, test_x, test_y, batch_size=32, num_epochs=100, learning_rate=0.001, weight_decay_factor=0.01, loss_type='L1', weight_decay_form='L1', momentum=False, momentum_factor=None)\n",
        "num_epochs=100\n",
        "plt.plot(range(num_epochs), train_loss_lasso,'r', label=\"Train loss\")\n",
        "plt.plot(range(num_epochs), test_loss_lasso, 'bo',label=\"Test loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "erfbyKAdDhYL",
        "outputId": "6f3af092-2f42-48d3-f7fe-f01247ed125d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 /100, Train loss: 1536.0170531858057 Test loss: 1536.126899846275\n",
            "Epoch: 2 /100, Train loss: 1072.4479731864324 Test loss: 1072.5578198469013\n",
            "Epoch: 3 /100, Train loss: 608.8788931871475 Test loss: 608.9887398476164\n",
            "Epoch: 4 /100, Train loss: 145.30981318786226 Test loss: 145.41965984833132\n",
            "Epoch: 5 /100, Train loss: 6.5264703281596494 Test loss: 6.54889155776134\n",
            "Epoch: 6 /100, Train loss: 6.53058675358964 Test loss: 6.541458864460236\n",
            "Epoch: 7 /100, Train loss: 6.5328863174135225 Test loss: 6.543642522543737\n",
            "Epoch: 8 /100, Train loss: 6.524030215931672 Test loss: 6.536205762485475\n",
            "Epoch: 9 /100, Train loss: 6.537786488604928 Test loss: 6.563258804596527\n",
            "Epoch: 10 /100, Train loss: 6.531281739970744 Test loss: 6.547503258074349\n",
            "Epoch: 11 /100, Train loss: 6.534989070496271 Test loss: 6.554967066901637\n",
            "Epoch: 12 /100, Train loss: 6.531095532992763 Test loss: 6.556480301049497\n",
            "Epoch: 13 /100, Train loss: 6.524887764207581 Test loss: 6.535603956175979\n",
            "Epoch: 14 /100, Train loss: 6.533693704763886 Test loss: 6.547261221622665\n",
            "Epoch: 15 /100, Train loss: 6.5422800420845695 Test loss: 6.549621977489756\n",
            "Epoch: 16 /100, Train loss: 6.544820838345437 Test loss: 6.568251175600791\n",
            "Epoch: 17 /100, Train loss: 6.53335053126893 Test loss: 6.546328763531058\n",
            "Epoch: 18 /100, Train loss: 6.527087680921837 Test loss: 6.538206473738469\n",
            "Epoch: 19 /100, Train loss: 6.524332045333225 Test loss: 6.525424631418786\n",
            "Epoch: 20 /100, Train loss: 6.528922147112968 Test loss: 6.551225498015264\n",
            "Epoch: 21 /100, Train loss: 6.534188250293096 Test loss: 6.540799687302863\n",
            "Epoch: 22 /100, Train loss: 6.527146465268671 Test loss: 6.551207941790709\n",
            "Epoch: 23 /100, Train loss: 6.53026515685106 Test loss: 6.5377889780248974\n",
            "Epoch: 24 /100, Train loss: 6.522974127268532 Test loss: 6.527907123189198\n",
            "Epoch: 25 /100, Train loss: 6.523164253110567 Test loss: 6.53699916993267\n",
            "Epoch: 26 /100, Train loss: 6.522739865951358 Test loss: 6.545494884932836\n",
            "Epoch: 27 /100, Train loss: 6.5305667806855165 Test loss: 6.5485972311230904\n",
            "Epoch: 28 /100, Train loss: 6.530996999050679 Test loss: 6.5435946940752325\n",
            "Epoch: 29 /100, Train loss: 6.5442456002114 Test loss: 6.566842517830664\n",
            "Epoch: 30 /100, Train loss: 6.547193878478004 Test loss: 6.580686232115595\n",
            "Epoch: 31 /100, Train loss: 6.519760117293381 Test loss: 6.533549192014146\n",
            "Epoch: 32 /100, Train loss: 6.528829808512611 Test loss: 6.534194034992112\n",
            "Epoch: 33 /100, Train loss: 6.531835903638616 Test loss: 6.541818655054623\n",
            "Epoch: 34 /100, Train loss: 6.528889702727949 Test loss: 6.541292179937882\n",
            "Epoch: 35 /100, Train loss: 6.531353310854548 Test loss: 6.553451655406749\n",
            "Epoch: 36 /100, Train loss: 6.535612979243516 Test loss: 6.561167701866685\n",
            "Epoch: 37 /100, Train loss: 6.541075709660231 Test loss: 6.55116774674926\n",
            "Epoch: 38 /100, Train loss: 6.52740705191481 Test loss: 6.541896873266768\n",
            "Epoch: 39 /100, Train loss: 6.532986334831917 Test loss: 6.550729728273784\n",
            "Epoch: 40 /100, Train loss: 6.530601777437011 Test loss: 6.553426304971033\n",
            "Epoch: 41 /100, Train loss: 6.524125535673144 Test loss: 6.538077699811003\n",
            "Epoch: 42 /100, Train loss: 6.531250244334073 Test loss: 6.5395710270369705\n",
            "Epoch: 43 /100, Train loss: 6.522650022966118 Test loss: 6.541552091325073\n",
            "Epoch: 44 /100, Train loss: 6.531810623441798 Test loss: 6.548813878466016\n",
            "Epoch: 45 /100, Train loss: 6.530355561243878 Test loss: 6.54665366930487\n",
            "Epoch: 46 /100, Train loss: 6.53231980145456 Test loss: 6.5436238327967615\n",
            "Epoch: 47 /100, Train loss: 6.520648323576632 Test loss: 6.538998764882419\n",
            "Epoch: 48 /100, Train loss: 6.530509503032264 Test loss: 6.535798921325833\n",
            "Epoch: 49 /100, Train loss: 6.52571830382196 Test loss: 6.540552655840758\n",
            "Epoch: 50 /100, Train loss: 6.527533352169106 Test loss: 6.544330607474175\n",
            "Epoch: 51 /100, Train loss: 6.526367393532242 Test loss: 6.535166297111982\n",
            "Epoch: 52 /100, Train loss: 6.528540239198301 Test loss: 6.542645574016687\n",
            "Epoch: 53 /100, Train loss: 6.5233473885037805 Test loss: 6.529005429415585\n",
            "Epoch: 54 /100, Train loss: 6.529644254332669 Test loss: 6.542591850361952\n",
            "Epoch: 55 /100, Train loss: 6.525295879418745 Test loss: 6.529079411213625\n",
            "Epoch: 56 /100, Train loss: 6.523294526203708 Test loss: 6.538948657790567\n",
            "Epoch: 57 /100, Train loss: 6.547147054074823 Test loss: 6.589075673558282\n",
            "Epoch: 58 /100, Train loss: 6.531936761014073 Test loss: 6.5421287710779685\n",
            "Epoch: 59 /100, Train loss: 6.526735698524981 Test loss: 6.540441878685221\n",
            "Epoch: 60 /100, Train loss: 6.535669164746907 Test loss: 6.553167755473588\n",
            "Epoch: 61 /100, Train loss: 6.542149923482794 Test loss: 6.565574365997749\n",
            "Epoch: 62 /100, Train loss: 6.528934430701656 Test loss: 6.543400150336026\n",
            "Epoch: 63 /100, Train loss: 6.527835402279244 Test loss: 6.531803319542795\n",
            "Epoch: 64 /100, Train loss: 6.529357213042905 Test loss: 6.538261257988557\n",
            "Epoch: 65 /100, Train loss: 6.532177357500746 Test loss: 6.5361513095089405\n",
            "Epoch: 66 /100, Train loss: 6.539000986608864 Test loss: 6.537759396980212\n",
            "Epoch: 67 /100, Train loss: 6.523583073120556 Test loss: 6.540951461300762\n",
            "Epoch: 68 /100, Train loss: 6.525113470466054 Test loss: 6.537070652114335\n",
            "Epoch: 69 /100, Train loss: 6.528017811018495 Test loss: 6.549129111578604\n",
            "Epoch: 70 /100, Train loss: 6.524051182638078 Test loss: 6.540516283675882\n",
            "Epoch: 71 /100, Train loss: 6.530068820992952 Test loss: 6.55599610894622\n",
            "Epoch: 72 /100, Train loss: 6.5283110299696565 Test loss: 6.554738677215939\n",
            "Epoch: 73 /100, Train loss: 6.5308262816259015 Test loss: 6.545133846943737\n",
            "Epoch: 74 /100, Train loss: 6.525316366889895 Test loss: 6.5402449881526685\n",
            "Epoch: 75 /100, Train loss: 6.520259606748006 Test loss: 6.536509227651458\n",
            "Epoch: 76 /100, Train loss: 6.540702464432996 Test loss: 6.543796549739866\n",
            "Epoch: 77 /100, Train loss: 6.5270566067010884 Test loss: 6.545083273269772\n",
            "Epoch: 78 /100, Train loss: 6.541351743387049 Test loss: 6.563196953932767\n",
            "Epoch: 79 /100, Train loss: 6.539525438736805 Test loss: 6.57609466258876\n",
            "Epoch: 80 /100, Train loss: 6.527507711813786 Test loss: 6.540391084869431\n",
            "Epoch: 81 /100, Train loss: 6.539895015214585 Test loss: 6.559475745581106\n",
            "Epoch: 82 /100, Train loss: 6.529628909809939 Test loss: 6.543486773877722\n",
            "Epoch: 83 /100, Train loss: 6.529936525637437 Test loss: 6.531167411536945\n",
            "Epoch: 84 /100, Train loss: 6.529893600408256 Test loss: 6.541647273729906\n",
            "Epoch: 85 /100, Train loss: 6.527165187617122 Test loss: 6.534152839860036\n",
            "Epoch: 86 /100, Train loss: 6.5239852887566085 Test loss: 6.535556730543647\n",
            "Epoch: 87 /100, Train loss: 6.528268032600441 Test loss: 6.547015723176053\n",
            "Epoch: 88 /100, Train loss: 6.525262637413785 Test loss: 6.543847708896491\n",
            "Epoch: 89 /100, Train loss: 6.524470509309285 Test loss: 6.5399328695250505\n",
            "Epoch: 90 /100, Train loss: 6.524158311975442 Test loss: 6.54072271624586\n",
            "Epoch: 91 /100, Train loss: 6.53771420705583 Test loss: 6.542925424789018\n",
            "Epoch: 92 /100, Train loss: 6.537713945781747 Test loss: 6.567585195468077\n",
            "Epoch: 93 /100, Train loss: 6.52315784442677 Test loss: 6.540257355842617\n",
            "Epoch: 94 /100, Train loss: 6.5305393117355495 Test loss: 6.549831190390056\n",
            "Epoch: 95 /100, Train loss: 6.541048327714148 Test loss: 6.5514503285235515\n",
            "Epoch: 96 /100, Train loss: 6.532084135088795 Test loss: 6.552636686304245\n",
            "Epoch: 97 /100, Train loss: 6.538444766621092 Test loss: 6.551582493984388\n",
            "Epoch: 98 /100, Train loss: 6.534801630428795 Test loss: 6.5461938248978875\n",
            "Epoch: 99 /100, Train loss: 6.528713342629401 Test loss: 6.543943651915869\n",
            "Epoch: 100 /100, Train loss: 6.526872842724135 Test loss: 6.542142838939532\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Count Regression\n",
        "\\begin{align*}\n",
        "y_{pred} &= e^{w^T X}\\\\\n",
        "L(y, y_{pred}) &= e^{-y_{pred}} y_{pred}^y / y!\\\\\n",
        "&= -\\sum_{i=1}^{n}(y_i log(y_{pred,i}) - y_{pred,i})\\\\\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "nCdeXttIDv23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_regression(X_train, y_train, X_test, y_test, batch_size=16, num_epochs=100, learning_rate=0.01, weight_decay_factor=0, loss_type='count', weight_decay_form='none', momentum=False, momentum_factor=0.9):\n",
        "    num_features = X_train.shape[1]\n",
        "    num_batches = int(np.ceil(len(X_train) / batch_size))\n",
        "    weight = np.random.normal(size=num_features)\n",
        "    m = 0\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    def forward(X, w):\n",
        "      # Clip X * w to avoid overflow in the exponential function\n",
        "      Xw = np.clip(np.dot(X, w), -100, None)\n",
        "      # Apply the exponential function and replace any resulting inf or nan values\n",
        "      y_pred = np.where(np.isinf(np.exp(Xw)), 1e10, np.exp(np.clip(Xw, -500, 500)))\n",
        "      return y_pred\n",
        "\n",
        "    def backward(X, error):\n",
        "        return np.dot(X.T, error)\n",
        "\n",
        "    def compute_gradient(X, y, y_pred, loss_type, w):\n",
        "      error = None\n",
        "      if loss_type == \"L2\":\n",
        "          error = 2*(y_pred - y)\n",
        "      elif loss_type == \"count\":\n",
        "          # Clip y_pred to avoid overflow\n",
        "          y_pred_clipped = np.clip(y_pred, -100, None)\n",
        "          error = np.exp(-y_pred_clipped) * (y_pred - y)\n",
        "      elif loss_type == \"cross-entropy\":\n",
        "          error = y_pred - y\n",
        "\n",
        "      gradient = backward(X, error)\n",
        "      if weight_decay_form == 'L2':\n",
        "          gradient += weight_decay_factor * w\n",
        "      elif weight_decay_form == 'L1':\n",
        "          gradient += weight_decay_factor * np.sign(w)\n",
        "\n",
        "      return gradient, error\n",
        "\n",
        "\n",
        "    def compute_loss(y, y_pred, loss_type):\n",
        "      if loss_type == \"L2\":\n",
        "          return np.mean(np.square(y - y_pred))\n",
        "      elif loss_type == \"count\":\n",
        "          # Clip y_pred to avoid overflow\n",
        "          y_pred_clipped = np.clip(y_pred, -100, None)\n",
        "          return np.mean(np.exp(-y_pred_clipped) * np.square(y - y_pred))\n",
        "      elif loss_type == \"cross-entropy\":\n",
        "          y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # clip predictions\n",
        "          return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "      elif loss_type == 'L1':\n",
        "          return np.mean(np.abs(y - y_pred))\n",
        "      else:\n",
        "          raise ValueError(\"Invalid loss type: {}\".format(loss_type))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle the data\n",
        "        perm = np.random.permutation(len(X_train))\n",
        "        X_train = X_train[perm]\n",
        "        y_train = y_train[perm]\n",
        "\n",
        "        # Mini-batch gradient descent\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = (i + 1) * batch_size\n",
        "            X_batch = X_train[start_idx:end_idx]\n",
        "            y_batch = y_train[start_idx:end_idx]\n",
        "\n",
        "            y_pred = forward(X_batch, weight)\n",
        "            gradient, error = compute_gradient(X_batch, y_batch, y_pred, loss_type, weight)\n",
        "\n",
        "            if momentum:\n",
        "                m = momentum_factor * m + (1 - momentum_factor) * gradient\n",
        "                weight -= learning_rate * m\n",
        "            else:\n",
        "                weight -= learning_rate * gradient\n",
        "\n",
        "        # Compute train and test losses\n",
        "        y_train_pred = forward(X_train, weight)\n",
        "        train_loss = compute_loss(y_train, y_train_pred, loss_type)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        y_test_pred = forward(X_test, weight)\n",
        "        test_loss = compute_loss(y_test, y_test_pred, loss_type)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        print(\"Epoch:\", epoch+1, \"/100, Train loss:\", train_loss, \"Test loss:\", test_loss)\n",
        "\n",
        "        # Check if the test loss has stopped decreasing\n",
        "        # if len(test_losses) >= 2 and test_losses[-1] >= test_losses[-2]:\n",
        "        #     print(\"Test loss has stopped decreasing. Stopping training.\")\n",
        "        #     break\n",
        "\n",
        "    return weight, train_losses, test_losses"
      ],
      "metadata": {
        "id": "cNIIYX9ZDrWg"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w, train_loss, test_loss= count_regression(train_x, train_y, test_x, test_y, batch_size=32, num_epochs=100, learning_rate=0.0001, weight_decay_factor=0.001, loss_type='L2', weight_decay_form='L2', momentum=False, momentum_factor=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "QxuLKLGkG1m3",
        "outputId": "d1d55d2e-15ef-4bc4-ad72-b1a7289492bd"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-132-ba956dda3c89>:13: RuntimeWarning: overflow encountered in exp\n",
            "  y_pred = np.where(np.isinf(np.exp(Xw)), 1e10, np.exp(np.clip(Xw, -500, 500)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 /100, Train loss: 2156500777401413.0 Test loss: 5810460385935185.0\n",
            "Epoch: 2 /100, Train loss: 1725200626014999.0 Test loss: 9684100640820816.0\n",
            "Epoch: 3 /100, Train loss: 646950236248892.4 Test loss: 3873640255393457.5\n",
            "Epoch: 4 /100, Train loss: 3993666.588416567 Test loss: 1936820130274826.5\n",
            "Epoch: 5 /100, Train loss: 3993666.588416567 Test loss: 1936820130274826.5\n",
            "Epoch: 6 /100, Train loss: 3993666.588416567 Test loss: 1936820130274826.5\n",
            "Epoch: 7 /100, Train loss: 3993666.588416567 Test loss: 1936820130274826.5\n",
            "Epoch: 8 /100, Train loss: 3993666.588416567 Test loss: 1936820130274826.5\n",
            "Epoch: 9 /100, Train loss: 3993666.588416567 Test loss: 1936820130274826.5\n",
            "Epoch: 10 /100, Train loss: 3993666.588416567 Test loss: 1936820130274826.5\n",
            "Epoch: 11 /100, Train loss: 3993666.588416567 Test loss: 1936820130274826.5\n",
            "Epoch: 12 /100, Train loss: 3993666.588416567 Test loss: 1936820130274826.5\n",
            "Epoch: 13 /100, Train loss: 3993666.588416567 Test loss: 1936820130274826.5\n",
            "Epoch: 14 /100, Train loss: 3993666.588416567 Test loss: 1936820130274826.5\n",
            "Epoch: 15 /100, Train loss: 3993666.588416567 Test loss: 1936820130274826.5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-f9a3409cd193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcount_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay_form\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-132-ba956dda3c89>\u001b[0m in \u001b[0;36mcount_regression\u001b[0;34m(X_train, y_train, X_test, y_test, batch_size, num_epochs, learning_rate, weight_decay_factor, loss_type, weight_decay_form, momentum, momentum_factor)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Shuffle the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=100\n",
        "plt.plot(range(num_epochs), train_loss,'r', label=\"Train loss\")\n",
        "plt.plot(range(num_epochs), test_loss, 'bo',label=\"Test loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "zs36ofwaG9q_",
        "outputId": "33911a1c-ee62-4164-e594-77bd9b96f1e7"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-f23891a235d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Train loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Test loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2766\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2767\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2768\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2769\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \"\"\"\n\u001b[1;32m   1634\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1636\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    499\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100,) and (3,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Model comparison\n"
      ],
      "metadata": {
        "id": "m0zHKAuX2wv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fig, ax = plt.subplots(figsize=(8, 6))\n",
        "plt.hist(w_ridge, alpha=0.5, label='Ridge')\n",
        "plt.xlabel('Weight Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Weights - Ridge')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "iYbX4Db7jqkq",
        "outputId": "08e6653e-b37a-4194-8deb-390417fb2dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(w_lasso, alpha=0.5, label='Lasso')\n",
        "plt.xlabel('Weight Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Weights - Lasso')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "kyqJJoVMEXKx",
        "outputId": "9970c72a-9a12-4080-9952-9b8bb4235d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot a histogram of the weights for the ridge, lasso, and count regression models. Discuss\n",
        "how the weights differ**\n",
        "\n",
        "For Ridge and Lasso regression, the weight is quite similar. It means that the penalty term applied is not really significant to the model.\n",
        "\n",
        "Lasso regression gives significantly lower loss. It's important to note that Lasso is better at handling multicollinearity by performing feature selection and shrinking the cofficient towards zero and more effective in handling outliers. This might indicates our data has high sparsity or multicollinearity which resulting in Lasso performs better than Ridge.  However, since the weights are similar, it means that both Ridge and Lasso are able to capture relationships between the attributes and target varible."
      ],
      "metadata": {
        "id": "h8iRKY2fo7gS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discuss and compare the behaviors of the models. Are there certain periods (ranges of\n",
        "years) in which models perform better than others? Where are the largest errors across\n",
        "models. Did regularization help for some models but not others?**\n"
      ],
      "metadata": {
        "id": "AGiCpk5-u0Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate model L2\n",
        "periods = [(1920, 1940), (1940, 1960), (1960, 1980), (1980, 2000), (2000, 2020)]\n",
        "\n",
        "for period in periods:\n",
        "    train_mask = (train_y >= period[0]) & (train_y < period[1])\n",
        "    test_mask = (test_y >= period[1]) & (test_y < period[1]+20)\n",
        "    train_x_period, train_y_period = train_x[train_mask], train_y[train_mask]\n",
        "    test_x_period, test_y_period = test_x[test_mask], test_y[test_mask]\n",
        "\n",
        "    # Train the model using mini-batch gradient descent\n",
        "    weight, train_losses, test_losses = mini_batch_gradient_descent(train_x_period, train_y_period, test_x_period, test_y_period, num_epochs=100, learning_rate=0.01, weight_decay_factor=0.001, loss_type='L2', weight_decay_form='L2')\n",
        "    \n",
        "    # Evaluate the performance of the model on the test set for the period\n",
        "    y_pred = np.dot(test_x_period, weight)\n",
        "    mse = np.mean(np.square(test_y_period - y_pred))\n",
        "    print(\"Period:\", period, \"MSE:\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "7qZdHQWqvktT",
        "outputId": "01122edf-f388-42b5-c56f-2b6b4fb6c611"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-4b56fadad6a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Train the model using mini-batch gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini_batch_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay_form\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Evaluate the performance of the model on the test set for the period\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-c850454e92f8>\u001b[0m in \u001b[0;36mmini_batch_gradient_descent\u001b[0;34m(X_train, y_train, X_test, y_test, batch_size, num_epochs, learning_rate, weight_decay_factor, loss_type, weight_decay_form, momentum, momentum_factor)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mtest_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-c850454e92f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(X, w)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (336,91) and (90,) not aligned: 91 (dim 1) != 90 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:, 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo5wVyN-zdBe",
        "outputId": "7cc79ab1-e4f8-4d84-c90e-cb059089afe2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         2001\n",
              "1         2001\n",
              "2         2001\n",
              "3         2001\n",
              "4         2001\n",
              "          ... \n",
              "515340    2006\n",
              "515341    2006\n",
              "515342    2006\n",
              "515343    2006\n",
              "515344    2005\n",
              "Name: target, Length: 515345, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate L2\n",
        "y_train_pred = np.dot(train_x, w_ridge)\n",
        "train_rmse = np.sqrt(np.mean(np.square(train_y - y_train_pred)))\n",
        "y_test_pred = np.dot(test_x, w_ridge)\n",
        "test_rmse = np.sqrt(np.mean(np.square(test_y - y_test_pred)))\n",
        "print(\"Train RMSE:\", train_rmse)\n",
        "print(\"Test RMSE:\", test_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSWXN3PMvFAP",
        "outputId": "54ef64ad-d978-40a4-a367-9d82d67dfe75"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train RMSE: 9.552927711558576\n",
            "Test RMSE: 9.512634607284436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate L1\n",
        "y_train_pred = np.dot(train_x, w_lasso)\n",
        "train_rmse = np.sqrt(np.mean(np.square(train_y - y_train_pred)))\n",
        "y_test_pred = np.dot(test_x, w_lasso)\n",
        "test_rmse = np.sqrt(np.mean(np.square(test_y - y_test_pred)))\n",
        "print(\"Train RMSE:\", train_rmse)\n",
        "print(\"Test RMSE:\", test_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUi6LwWvqyY5",
        "outputId": "1b8f3d19-7c49-45a7-c601-efc17fcbd075"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train RMSE: 9.870099705457289\n",
            "Test RMSE: 9.791397438648032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From RMSE above, L1 Lasso Regression gives larger error compared to Ridge."
      ],
      "metadata": {
        "id": "lMsyKIIZwOXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 5 - Softmax Properties\n",
        "\n",
        "\n",
        "###1. Show that the softmax function is invariant to constant offsets to its input\n",
        "\n",
        "\\begin{align*}\n",
        "softmax(a+c1) &= \\frac{exp(a+c1)}{\\sum_{j=1}^{n}exp(a_j+c)} \\\\\n",
        "&= \\frac{exp(a)exp(c1)}{\\sum_{j=1}^{n}exp(a_j)exp(c)}\\\\\n",
        "&= \\frac{exp(a)}{\\sum_{j=1}^{n}exp(a_j)} \\times \\frac{exp(c1)}{\\sum_{j=1}^{n}exp(c)}{}\\\\\n",
        "&= softmax(a) \\times \\frac{exp(c1)}{n \\times exp(c)}{}\\\\\n",
        "&= softmax(a) \\times \\frac{exp(c1-c)}{n}\n",
        "\\end{align*}\n",
        "\n",
        "Since $\\frac{exp(c1-c)}{n}$ is a constant, it doesn't depend on $a$ and it's proven that softmax function is invariant to constant offset to its input.\n"
      ],
      "metadata": {
        "id": "Ri2DQfSOEA6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.  Why is the observation that the softmax function is invariant to constant offsets to its input important when implementing it in a neural network?\n",
        "\n",
        "It's important that softmax function is invariant to constant offset to its input because it allows the model to be more robust to changes in input data. Adding or substracting a constant value from input data doesn't change the output of sofmax function and making the network to generalize better to new data that can improve reliability and accuracy."
      ],
      "metadata": {
        "id": "Pl2YQkyDNU7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 6 - Implementing Softmax Classifier\n",
        "\n",
        "Write a function to load the data and the labels, which are returned as NumPy arrays."
      ],
      "metadata": {
        "id": "d7XFmHfPP_t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXxYofx33qwQ",
        "outputId": "5a34087b-c7a2-46a1-bc16-09164de83ab6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_iris_data(train_file, test_file):\n",
        "    train_data = np.loadtxt(train_file)\n",
        "    test_data = np.loadtxt(test_file)\n",
        "    train_labels = train_data[:, 0].astype(int)\n",
        "    train_features = train_data[:, 1:]\n",
        "    test_labels = test_data[:, 0].astype(int)\n",
        "    test_features = test_data[:, 1:]\n",
        "    return train_features, train_labels, test_features, test_labels\n",
        "\n",
        "iris_train = '/content/drive/MyDrive/Deep Learning/HW2/iris-train.txt'\n",
        "iris_test = '/content/drive/MyDrive/Deep Learning/HW2/iris-test.txt'\n",
        "\n",
        "train_features, train_labels, test_features, test_labels = load_iris_data(iris_train, iris_test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_features, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxrvhNu63P7s",
        "outputId": "e29bbe7c-252d-4851-a425-a99bf482d78a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(72, 2)\n",
            "(18, 2)\n",
            "(72,)\n",
            "(18,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Implementation & Evaluation\n",
        "\n",
        "* Use softmax loss with L2 weight decay regularization  \n",
        "*  Use stochastic gradient descent with mini batches and momentum to minimize softmax loss of single LNN (use environment's BLAS)\n",
        "* Loop over epochs and mini batches (not individual vector/ matrices)\n",
        "* 1000 epochs\n",
        "* Normalize feature between -1 and 1\n",
        "* Initial weight from Gaussian distribution\n",
        "\n",
        "*Cross entropy loss*\n",
        "\n",
        "$y_{i,j}$: true label for the $i$-th sample and $j$-th class\n",
        "\n",
        "$p_{i,j}$: predicted probability for the $i$-th sample and $j$-th class. \n",
        "$$ L = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^C y_{i,j} \\log(p_{i,j}) $$\n",
        "\n",
        "\n",
        "What is the best test accuracy your model achieved? What hyperparameters did you use?\n",
        "Would early stopping have helped improve accuracy on the test data?\n",
        "\n"
      ],
      "metadata": {
        "id": "TG4ZLs-d3nDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize feature to be between -1 and 1\n",
        "X_train= (X_train - np.mean(X_train, axis=0)) / (np.std(X_train, axis=0)+ 1e-8)\n",
        "X_test= (X_test - np.mean(X_test, axis=0)) / (np.std(X_test, axis=0)+1e-8)\n",
        "\n",
        "#Add bias to the features\n",
        "X_train= np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
        "X_test= np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
        "\n",
        "def mini_batch_gradient_descent(train_x, train_y, test_x, test_y, num_epochs=1000, batch_size=32, learning_rate=0.01, momentum_rate=0.9, weight_decay_factor=0.01):\n",
        "  num_classes = len(np.unique(train_y))\n",
        "  W = np.random.normal(size=(train_x.shape[1], num_classes))\n",
        "  v = np.zeros_like(W)\n",
        "\n",
        "  def softmax(X):\n",
        "    exp_X = np.exp(X)\n",
        "    return exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
        "  \n",
        "  def cross_entropy_loss(y_true, y_pred, W):\n",
        "    num_samples = y_true.shape[0]\n",
        "    data_loss = -np.sum(np.log(y_pred[range(num_samples), y_true]))\n",
        "    reg_loss = 0.5 * weight_decay_factor * np.sum(W**2)\n",
        "    return (data_loss + reg_loss) / num_samples\n",
        "  \n",
        "  # Define the function to compute the gradients of the loss with respect to the weights\n",
        "  def compute_gradients(X, y_true, y_pred, W):\n",
        "      num_samples = y_true.shape[0]\n",
        "      grad_data_loss = y_pred\n",
        "      grad_data_loss[range(num_samples), y_true] -= 1\n",
        "      grad_data_loss /= num_samples\n",
        "      grad_W = np.dot(X.T, grad_data_loss) + weight_decay_factor * W\n",
        "      return grad_W\n",
        "  \n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  train_accs = []\n",
        "  test_accs = []\n",
        "\n",
        "  # Loop over the epochs\n",
        "  for epoch in range(num_epochs):\n",
        "    # Shuffle the data\n",
        "    perm = np.random.permutation(train_X.shape[0])\n",
        "    train_X = train_X[perm]\n",
        "    train_y = train_y[perm]\n",
        "\n",
        "    # Loop over the batches\n",
        "    for i in range(0, train_X.shape[0], batch_size):\n",
        "        batch_X = train_X[i:i+batch_size]\n",
        "        batch_y = train_y[i:i+batch_size]\n",
        "        batch_pred = softmax(np.dot(batch_X, W)) #Compute y_predict\n",
        "        grad_W = compute_gradients(batch_X, batch_y, batch_pred, W) #Compute gradient\n",
        "\n",
        "        # Update weights with momentum\n",
        "        v = momentum_rate * v - learning_rate * grad_W\n",
        "        W += v\n",
        "\n",
        "    # Compute the predictions and losses\n",
        "    train_pred = softmax(np.dot(train_x, W))\n",
        "    train_loss = cross_entropy_loss(train_y, train_pred, W)\n",
        "    train_losses.append(train_loss)\n",
        "    train_acc = np.mean(train_y == np.argmax(train_pred, axis=1))\n",
        "    train_accs.append(train_acc)\n",
        "    test_pred = softmax(np.dot(test_x, W))\n",
        "    test_loss = cross_entropy_loss(test_y, test_pred, W)\n",
        "    test_losses.append(test_loss)\n",
        "    test_acc = np.mean(test_y == np.argmax(test_pred, axis=1))\n",
        "    test_accs.append(test_acc)\n",
        "  \n",
        "  return train_losses, train_accs, test_losses, test_accs\n"
      ],
      "metadata": {
        "id": "501xlYqs2m77"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5l25TrLxIkcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, train_accuracies, test_losses, test_accuracies= mini_batch_gradient_descent(X_train, y_train, X_test, y_test, num_epochs=1000, batch_size=32, learning_rate=0.01, momentum_rate=0.9, weight_decay_factor=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "CsrvBqRB_rpZ",
        "outputId": "8a80e7c0-29e6-4d1b-b3f9-7efdb2d93c6a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-123a019cdc95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracies\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmini_batch_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-68-5e7614b9e71e>\u001b[0m in \u001b[0;36mmini_batch_gradient_descent\u001b[0;34m(train_x, train_y, test_x, test_y, num_epochs, batch_size, learning_rate, momentum_rate, weight_decay_factor)\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Shuffle the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mtrain_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'train_X' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plot the cross-entropy loss during training for both the train and test sets\n",
        "ax1.plot(range(num_epochs), train_losses, label='Train')\n",
        "ax1.plot(range(num_epochs), test_losses, label='Test')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Cross-entropy loss')\n",
        "ax1.set_title('Training and test loss')\n",
        "ax1.legend()\n",
        "\n",
        "# Plot the accuracy during training for both the train and test sets\n",
        "ax2.plot(range(num_epochs), train_accuracies, label='Train')\n",
        "ax2.plot(range(num_epochs), test_accuracies, label='Test')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Mean per-class accuracy')\n",
        "ax2.set_title('Training and test accuracy')\n",
        "ax2.legend()\n",
        "\n",
        "# Show the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JmxJUPiQ__DC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}