{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anastasiarenata1/deeplearning/blob/main/LNN_with_Numpy_and_Softmax_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRQriJO6Rw4D"
      },
      "source": [
        "# Problem 4 - Regression\n",
        "\n",
        "Classification data from 2011 Million Song Challenge dataset to predict music year\n",
        "\n",
        "* Explore three shallow (linear) neural network models with different activation functions for this task.\n",
        "* Evaluate the model by rounding the output of your linear neural network and compute the mean squared error\n",
        "\n",
        "\n",
        "###1. Load and explore the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEErmxZU2EDU",
        "outputId": "367194d3-32ec-4685-8bd4-9db4114b0e95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-02 04:33:15--  https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 211011981 (201M) [application/x-httpd-php]\n",
            "Saving to: ‘YearPredictionMSD.txt.zip’\n",
            "\n",
            "YearPredictionMSD.t 100%[===================>] 201.24M  17.3MB/s    in 13s     \n",
            "\n",
            "2023-03-02 04:33:29 (15.2 MB/s) - ‘YearPredictionMSD.txt.zip’ saved [211011981/211011981]\n",
            "\n",
            "Archive:  YearPredictionMSD.txt.zip\n",
            "  inflating: YearPredictionMSD.txt   \n"
          ]
        }
      ],
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n",
        "!unzip YearPredictionMSD.txt.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jYGlyolcU0ai"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "BbPalayaR_Kz",
        "outputId": "f27f0be8-78a1-44b9-905b-e7cec4fbe236"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   target  timbre_avg_1  timbre_avg_2  timbre_avg_3  timbre_avg_4  \\\n",
              "0    2001      49.94357      21.47114      73.07750       8.74861   \n",
              "1    2001      48.73215      18.42930      70.32679      12.94636   \n",
              "2    2001      50.95714      31.85602      55.81851      13.41693   \n",
              "3    2001      48.24750      -1.89837      36.29772       2.58776   \n",
              "4    2001      50.97020      42.20998      67.09964       8.46791   \n",
              "\n",
              "   timbre_avg_5  timbre_avg_6  timbre_avg_7  timbre_avg_8  timbre_avg_9  ...  \\\n",
              "0     -17.40628     -13.09905     -25.01202     -12.23257       7.83089  ...   \n",
              "1     -10.32437     -24.83777       8.76630      -0.92019      18.76548  ...   \n",
              "2      -6.57898     -18.54940      -3.27872      -2.35035      16.07017  ...   \n",
              "3       0.97170     -26.21683       5.05097     -10.34124       3.55005  ...   \n",
              "4     -15.85279     -16.81409     -12.48207      -9.37636      12.63699  ...   \n",
              "\n",
              "   timbre_covar_69  timbre_covar_70  timbre_covar_71  timbre_covar_72  \\\n",
              "0         13.01620        -54.40548         58.99367         15.37344   \n",
              "1          5.66812        -19.68073         33.04964         42.87836   \n",
              "2          3.03800         26.05866        -50.92779         10.93792   \n",
              "3         34.57337       -171.70734        -16.96705        -46.67617   \n",
              "4          9.92661        -55.95724         64.92712        -17.72522   \n",
              "\n",
              "   timbre_covar_73  timbre_covar_74  timbre_covar_75  timbre_covar_76  \\\n",
              "0          1.11144        -23.08793         68.40795         -1.82223   \n",
              "1         -9.90378        -32.22788         70.49388         12.04941   \n",
              "2         -0.07568         43.20130       -115.00698         -0.05859   \n",
              "3        -12.51516         82.58061        -72.08993          9.90558   \n",
              "4         -1.49237         -7.50035         51.76631          7.88713   \n",
              "\n",
              "   timbre_covar_77  timbre_covar_78  \n",
              "0        -27.46348          2.26327  \n",
              "1         58.43453         26.92061  \n",
              "2         39.67068         -0.66345  \n",
              "3        199.62971         18.85382  \n",
              "4         55.66926         28.74903  \n",
              "\n",
              "[5 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-940276d3-d9c0-40c8-820f-1b3f8904da41\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>timbre_avg_1</th>\n",
              "      <th>timbre_avg_2</th>\n",
              "      <th>timbre_avg_3</th>\n",
              "      <th>timbre_avg_4</th>\n",
              "      <th>timbre_avg_5</th>\n",
              "      <th>timbre_avg_6</th>\n",
              "      <th>timbre_avg_7</th>\n",
              "      <th>timbre_avg_8</th>\n",
              "      <th>timbre_avg_9</th>\n",
              "      <th>...</th>\n",
              "      <th>timbre_covar_69</th>\n",
              "      <th>timbre_covar_70</th>\n",
              "      <th>timbre_covar_71</th>\n",
              "      <th>timbre_covar_72</th>\n",
              "      <th>timbre_covar_73</th>\n",
              "      <th>timbre_covar_74</th>\n",
              "      <th>timbre_covar_75</th>\n",
              "      <th>timbre_covar_76</th>\n",
              "      <th>timbre_covar_77</th>\n",
              "      <th>timbre_covar_78</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 91 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-940276d3-d9c0-40c8-820f-1b3f8904da41')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-940276d3-d9c0-40c8-820f-1b3f8904da41 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-940276d3-d9c0-40c8-820f-1b3f8904da41');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "colnames = ['target'] + ['timbre_avg_' + str(i) for i in range(1, 13)] + ['timbre_covar_' + str(i) for i in range(1, 79)]\n",
        "df = pd.read_csv('YearPredictionMSD.txt', header=None, names=colnames)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqj7q70Ql5lb"
      },
      "source": [
        "Write a function to load the dataset, e.g.,\n",
        "`trainYears, trainFeat, testYears, testFeat = loadMusicData(fname, addBias)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HO0rnbz9STFd"
      },
      "outputs": [],
      "source": [
        "def loadMusicData(data, addBias=True):\n",
        "  train_df = data[:463714]\n",
        "  test_df = data[463714:]\n",
        "  train_y = train_df['target'].values\n",
        "  train_x = train_df.iloc[:,1:].values\n",
        "  test_y = test_df['target'].values\n",
        "  test_x = test_df.iloc[:,1:].values\n",
        "  if addBias:\n",
        "    train_x = np.hstack((train_x, np.ones((train_x.shape[0],1))))\n",
        "    test_x = np.hstack((test_x, np.ones((test_x.shape[0],1))))\n",
        "  return train_y, train_x, test_y, test_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz0sr2dbmDpu"
      },
      "source": [
        "Write a function `mse = musicMSE(pred, gt)` where the inputs are the predicted year and the “ground truth” year from the dataset. The function computes the mean squared error(MSE) by rounding pred before computing the MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eBi_cfQ4nGmY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "def musicMSE(pred, gt):\n",
        "  pred = np.round(pred)\n",
        "  mse= mean_squared_error(pred, gt)\n",
        "  return mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNpBvB08ojLl"
      },
      "source": [
        "Load the dataset and discuss its properties. \n",
        "1. What is the range of the variables? From 90 attributes, range of variables for timbre average is tighter than range of variables for timbre covariance. However, within each category itself, some attributes have wider range compared to others, in which we don't have further documentation to explain this event.\n",
        "2. How might you normalize them? Normalization can help to ensure that each variables contribute equally to the model. Since range of variables varies significantly accross 90 attributes, I will normalize the data using standardization technique (0 mean and unit std deviation for each attribute) where we can help to preserve importance of variables.\n",
        "3. What years are represented in the dataset? The dataset covers song released from 1922 to 2011 (90 years) with most common year of 2007.\n",
        "4. What will the test mean squared error (MSE) be if your classifier always outputs the most common year in the dataset? 190.08"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwHVPkNVoTmn",
        "outputId": "97173a41-b477-4bc0-c63f-c8823ab129b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variable 1: range = 60\n",
            "Variable 2: range = 721\n",
            "Variable 3: range = 624\n",
            "Variable 4: range = 490\n",
            "Variable 5: range = 444\n",
            "Variable 6: range = 248\n",
            "Variable 7: range = 361\n",
            "Variable 8: range = 199\n",
            "Variable 9: range = 273\n",
            "Variable 10: range = 102\n",
            "Variable 11: range = 158\n",
            "Variable 12: range = 182\n",
            "Variable 13: range = 550\n",
            "Variable 14: range = 65727\n",
            "Variable 15: range = 36796\n",
            "Variable 16: range = 31832\n",
            "Variable 17: range = 19854\n",
            "Variable 18: range = 16826\n",
            "Variable 19: range = 11882\n",
            "Variable 20: range = 9564\n",
            "Variable 21: range = 9610\n",
            "Variable 22: range = 3707\n",
            "Variable 23: range = 6731\n",
            "Variable 24: range = 9808\n",
            "Variable 25: range = 4871\n",
            "Variable 26: range = 37870\n",
            "Variable 27: range = 26522\n",
            "Variable 28: range = 7735\n",
            "Variable 29: range = 6635\n",
            "Variable 30: range = 6669\n",
            "Variable 31: range = 6153\n",
            "Variable 32: range = 3471\n",
            "Variable 33: range = 4567\n",
            "Variable 34: range = 3921\n",
            "Variable 35: range = 2803\n",
            "Variable 36: range = 4208\n",
            "Variable 37: range = 22597\n",
            "Variable 38: range = 18155\n",
            "Variable 39: range = 15869\n",
            "Variable 40: range = 16243\n",
            "Variable 41: range = 8211\n",
            "Variable 42: range = 8068\n",
            "Variable 43: range = 6178\n",
            "Variable 44: range = 2904\n",
            "Variable 45: range = 1768\n",
            "Variable 46: range = 2529\n",
            "Variable 47: range = 23921\n",
            "Variable 48: range = 10960\n",
            "Variable 49: range = 12815\n",
            "Variable 50: range = 4412\n",
            "Variable 51: range = 3698\n",
            "Variable 52: range = 4207\n",
            "Variable 53: range = 5583\n",
            "Variable 54: range = 5100\n",
            "Variable 55: range = 2263\n",
            "Variable 56: range = 12109\n",
            "Variable 57: range = 17812\n",
            "Variable 58: range = 17732\n",
            "Variable 59: range = 9176\n",
            "Variable 60: range = 8190\n",
            "Variable 61: range = 3711\n",
            "Variable 62: range = 2370\n",
            "Variable 63: range = 1413\n",
            "Variable 64: range = 21394\n",
            "Variable 65: range = 10254\n",
            "Variable 66: range = 7344\n",
            "Variable 67: range = 3254\n",
            "Variable 68: range = 7345\n",
            "Variable 69: range = 7192\n",
            "Variable 70: range = 1720\n",
            "Variable 71: range = 11016\n",
            "Variable 72: range = 11695\n",
            "Variable 73: range = 10525\n",
            "Variable 74: range = 3453\n",
            "Variable 75: range = 2666\n",
            "Variable 76: range = 1459\n",
            "Variable 77: range = 19852\n",
            "Variable 78: range = 5449\n",
            "Variable 79: range = 13578\n",
            "Variable 80: range = 8490\n",
            "Variable 81: range = 1279\n",
            "Variable 82: range = 8872\n",
            "Variable 83: range = 5021\n",
            "Variable 84: range = 4832\n",
            "Variable 85: range = 602\n",
            "Variable 86: range = 6831\n",
            "Variable 87: range = 7154\n",
            "Variable 88: range = 699\n",
            "Variable 89: range = 14852\n",
            "Variable 90: range = 1059\n"
          ]
        }
      ],
      "source": [
        "# Range of variables\n",
        "var_ranges = np.ptp(df.iloc[:, 1:].values, axis=0)\n",
        "for i, var_range in enumerate(var_ranges):\n",
        "    print(\"Variable {}: range = {:.0f}\".format(i+1, var_range))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6xcZrqpiyO0F"
      },
      "outputs": [],
      "source": [
        "#Normalize data using standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "def loadMusicData2(data, addBias=True):\n",
        "  train_df = data[:463714]\n",
        "  test_df = data[463714:]\n",
        "  train_y = train_df['target'].values\n",
        "  train_x = train_df.iloc[:,1:].values\n",
        "  test_y = test_df['target'].values\n",
        "  test_x = test_df.iloc[:,1:].values\n",
        "  train_x= scaler.fit_transform(train_x)\n",
        "  test_x= scaler.fit_transform(test_x)\n",
        "\n",
        "  if addBias:\n",
        "    train_x = np.hstack((train_x, np.ones((train_x.shape[0],1))))\n",
        "    test_x = np.hstack((test_x, np.ones((test_x.shape[0],1))))\n",
        "  \n",
        "\n",
        "  return train_y, train_x, test_y, test_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J88tbEW8y56c",
        "outputId": "03f341f4-ea2e-455a-98fa-95e1436a1f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min years presented:  1922\n",
            "Max years presented:  2011\n",
            "Median years presented:  2002.0\n",
            "Most common year: 2007\n"
          ]
        }
      ],
      "source": [
        "#Years represented\n",
        "print('Min years presented: ', np.min(df['target']))\n",
        "print('Max years presented: ', np.max(df['target']))\n",
        "print('Median years presented: ', np.median(df['target']))\n",
        "\n",
        "from statistics import mode\n",
        "print('Most common year:', mode(df['target']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XsuMY-B-6Rq8"
      },
      "outputs": [],
      "source": [
        "train_y, train_x, test_y, test_x = loadMusicData2(df, addBias=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgxWh8BG6E15",
        "outputId": "7461d868-e5cd-4052-f6bf-ccdaab1b4d10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "190.08239236117836"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "#test MSE\n",
        "musicMSE(torch.full((test_y.shape[0],), 2007), test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzi847OP89JV"
      },
      "source": [
        "##2. Classification\n",
        "This problem could have been posed as a classification problem by treating each year as a category. What would be the problems with this approach? Support your argument by analyzing a bar chart with the year as the x-axis and the number of examples for that year as the y-axis.\n",
        "\n",
        "As we can see from the chart, the distribution of train dataset is skewed to the left where majority of the data coming from the later years. If we treat this problem as a classification problem, the model will be biased and will be more likely to predict later years. Furthermore, classification means that the predicted data will be categorical values instead of continuous which can result in loss of information as repercussion. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "QByIY0eo3_uO",
        "outputId": "8ccfabf9-9dca-4e88-b3e5-1bb6f2753a2b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEGCAYAAACkQqisAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAa9UlEQVR4nO3df5QdZZ3n8feHECSgmETabEzCNGqUjSghNCG7oysDa+jAcRJXZMEZ0wdZ4h7CHj3rzBJcz4Iie8AzAzOMwmwcsiT+Coi6yUAwExkcjrObHx0I+QmTJgRJiKQlQASUGPjuH/W0VDq3O7crXff27ft5nVPnVn3rx32q+pIvTz1PPaWIwMzMrIhj6l0AMzNrXE4iZmZWmJOImZkV5iRiZmaFOYmYmVlhx9a7ALV28sknR2tra72LYWbWUNavX/+riGjpHW+6JNLa2kpnZ2e9i2Fm1lAkPV0p7ttZZmZWmJOImZkV5iRiZmaFOYmYmVlhTiJmZlaYk4iZmRXmJGJmZoU5iZiZWWGlJRFJx0taK+kxSVskfSXF75L0lKQNaZqa4pJ0m6QuSRslTcsdq0PS9jR15OJnSdqU9rlNkso6HzMzO1yZT6y/BpwXES9LGgn8XNIDad2fR8S9vbafBUxO0znAHcA5ksYC1wFtQADrJS2PiBfSNlcCa4AVQDvwAGZmw0zrgvt/P7/zpovqWJJDlVYTiczLaXFkmvp7jeJsYEnabzUwWtJ44AJgVUTsS4ljFdCe1p0UEasjez3jEmBOWedjZmaHK7VNRNIISRuAvWSJYE1adWO6ZXWrpLek2ATgmdzuu1Ksv/iuCvFK5ZgnqVNSZ3d399GelpmZJaUmkYh4PSKmAhOB6ZJOB64FTgPOBsYC15RZhlSOhRHRFhFtLS2HDUJpZmYF1aR3VkS8CDwEtEfEnnTL6jXgfwPT02a7gUm53SamWH/xiRXiZmZWI2X2zmqRNDrNjwI+Bjye2jJIPanmAJvTLsuBuamX1gzgpYjYA6wEZkoaI2kMMBNYmdbtlzQjHWsusKys8zEzs8OV2TtrPLBY0giyZHVPRNwn6R8ltQACNgD/OW2/ArgQ6AJeBS4HiIh9km4A1qXtvhoR+9L8VcBdwCiyXlnumWVmVkOlJZGI2AicWSF+Xh/bBzC/j3WLgEUV4p3A6UdXUjMzK8pPrJuZWWFOImZmQ1TrgvsPechwKHISMTOzwpxEzMysMCcRMzMrzEnEzMwKcxIxM7PCnETMzKwwJxEzMyvMScTMzApzEjEzs8KcRMzMrDAnETMzK8xJxMzMCnMSMTOzwsp8KZWZmQ1AfsTenTddVMeSVM9JxMyswQylZOPbWWZmVpiTiJmZFeYkYmZmhZWWRCQdL2mtpMckbZH0lRQ/VdIaSV2S7pZ0XIq/JS13pfWtuWNdm+JPSLogF29PsS5JC8o6FzMzq6zMmshrwHkRcQYwFWiXNAO4Gbg1It4LvABckba/AnghxW9N2yFpCnAp8AGgHbhd0ghJI4BvArOAKcBlaVszM6uR0pJIZF5OiyPTFMB5wL0pvhiYk+Znp2XS+vMlKcWXRsRrEfEU0AVMT1NXROyIiAPA0rStmZnVSKltIqnGsAHYC6wCngRejIiDaZNdwIQ0PwF4BiCtfwl4Rz7ea5++4pXKMU9Sp6TO7u7uQTgzMzODkpNIRLweEVOBiWQ1h9PK/L5+yrEwItoioq2lpaUeRTAzG5Zq0jsrIl4EHgL+DTBaUs9DjhOB3Wl+NzAJIK1/O/B8Pt5rn77iZmZWI2X2zmqRNDrNjwI+BmwjSyYXp806gGVpfnlaJq3/x4iIFL809d46FZgMrAXWAZNTb6/jyBrfl5d1PmZmdrgyhz0ZDyxOvaiOAe6JiPskbQWWSvoa8ChwZ9r+TuDbkrqAfWRJgYjYIukeYCtwEJgfEa8DSLoaWAmMABZFxJYSz8fMzHopLYlExEbgzArxHWTtI73jvwU+1cexbgRurBBfAaw46sKamVkhfmLdzMwKcxIxM7PCnETMzKwwJxEzMyvMScTMzApzEjEzs8L8elwzswZXz9fluiZiZmaFOYmYmVlhTiJmZlaYk4iZmRXmJGJmZoU5iZiZWWFOImZmVpifEzEzq6P8Mx6NyDURMzMrzEnEzMwKcxIxM7PCnETMzKwwJxEzMyustCQiaZKkhyRtlbRF0udT/HpJuyVtSNOFuX2uldQl6QlJF+Ti7SnWJWlBLn6qpDUpfrek48o6HzMzO1yZNZGDwBcjYgowA5gvaUpad2tETE3TCoC07lLgA0A7cLukEZJGAN8EZgFTgMtyx7k5Heu9wAvAFSWej5mZ9VJaEomIPRHxSJr/NbANmNDPLrOBpRHxWkQ8BXQB09PUFRE7IuIAsBSYLUnAecC9af/FwJxSTsbMzCqqSZuIpFbgTGBNCl0taaOkRZLGpNgE4JncbrtSrK/4O4AXI+Jgr3il758nqVNSZ3d392CckpmZUYMkIumtwA+BL0TEfuAO4D3AVGAP8JdllyEiFkZEW0S0tbS0lP11ZmZNo9RhTySNJEsg342IHwFExHO59d8C7kuLu4FJud0nphh9xJ8HRks6NtVG8tubmVkNlJZEUpvFncC2iLglFx8fEXvS4ieAzWl+OfA9SbcA7wImA2sBAZMlnUqWJC4FPh0RIekh4GKydpIOYFlZ52NmNhjq+T70MpRZE/lD4DPAJkkbUuxLZL2rpgIB7AQ+BxARWyTdA2wl69k1PyJeB5B0NbASGAEsiogt6XjXAEslfQ14lCxpmZlZjZSWRCLi52S1iN5W9LPPjcCNFeIrKu0XETvIem+ZmVkd+Il1MzMrzEnEzMwKcxIxM7PCnETMzKwwJxEzMyvMScTMzApzEjEzs8KcRMzMrDAnETMzK6yqJCLpg2UXxMzMGk+1NZHbJa2VdJWkt5daIjMzaxhVJZGI+AjwJ2RDsq+X9D1JHyu1ZGZmNuRV3SYSEduBL5ONnPtR4DZJj0v6D2UVzszMhrZq20Q+JOlWsveknwd8PCL+dZq/tcTymZnZEFbtUPB/A/wd8KWI+E1PMCKelfTlUkpmZmZDXrVJ5CLgN7mXRB0DHB8Rr0bEt0srnZmZDWnVJpGfAv8eeDktnwD8A/BvyyiUmdlwkn8l7nBTbcP68RHRk0BI8yeUUyQzM2sU1SaRVyRN61mQdBbwm362NzOzJlDt7awvAD+Q9CzZe9P/FfAfyyqUmZk1hqqSSESsk3Qa8P4UeiIifldesczMrBEMZADGs4EPAdOAyyTN7W9jSZMkPSRpq6Qtkj6f4mMlrZK0PX2OSXFJuk1Sl6SNvW6fdaTtt0vqyMXPkrQp7XObJA3k5M3M7OhU+7Dht4G/AD5MlkzOBtqOsNtB4IsRMQWYAcyXNAVYADwYEZOBB9MywCxgcprmAXek7x4LXAecA0wHrutJPGmbK3P7tVdzPmZmNjiqbRNpA6ZERFR74IjYA+xJ87+WtA2YAMwGzk2bLQZ+RjaUymxgSfqO1ZJGSxqftl0VEfsAJK0C2iX9DDgpIlan+BJgDvBAtWU0M7OjU+3trM1kjemFSGoFzgTWAONSggH4JTAuzU8AnsnttivF+ovvqhCv9P3zJHVK6uzu7i56GmZm1ku1NZGTga2S1gKv9QQj4o+PtKOktwI/BL4QEfvzzRYREZKqrt0UFRELgYUAbW1tpX+fmVmzqDaJXF/k4JJGkiWQ70bEj1L4OUnjI2JPul21N8V3kw0132Niiu3mzdtfPfGfpfjECtubmVmNVPs+kX8CdgIj0/w64JH+9kk9pe4EtkXELblVy4GeHlYdwLJcfG7qpTUDeCnd9loJzJQ0JjWozwRWpnX7Jc1I3zU3dywzs7pqXXD/sB7upEdVNRFJV5L1mBoLvIes7eFvgfP72e0Pgc8AmyRtSLEvATcB90i6AngauCStWwFcCHQBrwKXA0TEPkk3kCUugK/2NLIDVwF3AaPIGtTdqG5mVkPV3s6aT9a9dg1kL6iS9M7+doiIn5M93V7JYckn9cqa38exFgGLKsQ7gdP7LbmZmZWm2t5Zr0XEgZ4FSccCbqA2M2ty1SaRf5L0JWBUerf6D4C/L69YZmbWCKpNIguAbmAT8Dmy9gu/0dDMrMlVOwDjG8C30mRmZgZU3zvrKSq0gUTEuwe9RGZm1jAGMnZWj+OBT5F19zUzsyZW7cOGz+em3RHxV8BF5RbNzMyGumpvZ03LLR5DVjOpthZjZmbDVLWJ4C9z8wfJhkC5pPKmZmbWLKrtnfVHZRfEzKyR5cfJ2nlT89ztr/Z21n/tb32vARbNzKxJDKR31tlkI+0CfBxYC2wvo1BmZtYYqk0iE4FpEfFrAEnXA/dHxJ+WVTAzMxv6qh32ZBxwILd8gDdfa2tmZk2q2prIEmCtpB+n5TnA4lJKZGZmDaPa3lk3SnoA+EgKXR4Rj5ZXLDOzoa8Z3lx4JNXezgI4AdgfEX8N7JJ0akllMjOzBlFVEpF0HXANcG0KjQS+U1ahzMysMVRbE/kE8MfAKwAR8SzwtrIKZWZmjaHaJHIgvQM9ACSdWF6RzMysUVSbRO6R9L+A0ZKuBH7KEV5QJWmRpL2SNudi10vaLWlDmi7MrbtWUpekJyRdkIu3p1iXpAW5+KmS1qT43ZKOq/akzcxscBwxiUgScDdwL/BD4P3A/4iIvznCrncB7RXit0bE1DStSN8xBbgU+EDa53ZJIySNAL4JzAKmAJelbQFuTsd6L/ACcMWRzsXMzAbXEbv4RkRIWhERHwRWVXvgiHhYUmuVm88GlkbEa8BTkrqA6WldV0TsAJC0FJgtaRtwHvDptM1i4HrgjmrLZ2ZmR6/a21mPSDp7kL7zakkb0+2uMSk2AXgmt82uFOsr/g7gxYg42CtekaR5kjoldXZ3dw/SaZiZWbVJ5BxgtaQnUwLYJGljge+7A3gPMBXYw6HvKSlNRCyMiLaIaGtpaanFV5qZNYV+b2dJOiUifgFc0N921YqI53LH/hZwX1rcDUzKbToxxegj/jxZI/+xqTaS397MzGrkSDWR/wMQEU8Dt0TE0/lpoF8maXxu8RNAT8+t5cClkt6SnoSfTDbU/DpgcuqJdRxZ4/vy1N34IeDitH8HsGyg5TEzs6NzpIZ15ebfPZADS/o+cC5wsqRdwHXAuZKmkj1vshP4HEBEbJF0D7CV7PW78yPi9XScq4GVwAhgUURsSV9xDbBU0teAR4E7B1I+M7MiesbLaqa3F/bnSEkk+pg/ooi4rEK4z3/oI+JG4MYK8RXAigrxHbzZg8vMzOrgSEnkDEn7yWoko9I8aTki4qRSS2dmZkNav0kkIkbUqiBmZtZ4qn0plZlZU8q/M8TtIIcbyPtEzMzMDuEkYmZmhTmJmJlZYU4iZmZWmJOImZkV5iRiZtZL64L7D+mVZX1zEjEzs8KcRMzMrDAnETMzK8xJxMzMCvOwJ2bW9Dy0SXGuiZiZWWFOImZmVpiTiJmZFeYkYmZmhblh3cyakp9IHxyuiZiZWWGlJRFJiyTtlbQ5FxsraZWk7elzTIpL0m2SuiRtlDQtt09H2n67pI5c/CxJm9I+t0lSWediZmaVlVkTuQto7xVbADwYEZOBB9MywCxgcprmAXdAlnSA64BzgOnAdT2JJ21zZW6/3t9lZmYlKy2JRMTDwL5e4dnA4jS/GJiTiy+JzGpgtKTxwAXAqojYFxEvAKuA9rTupIhYHREBLMkdy8zMaqTWbSLjImJPmv8lMC7NTwCeyW23K8X6i++qEK9I0jxJnZI6u7u7j+4MzMzs9+rWsJ5qEFGj71oYEW0R0dbS0lKLrzQzawq1TiLPpVtRpM+9Kb4bmJTbbmKK9RefWCFuZmY1VOskshzo6WHVASzLxeemXlozgJfSba+VwExJY1KD+kxgZVq3X9KM1Ctrbu5YZmYV+Y2Fg6+0hw0lfR84FzhZ0i6yXlY3AfdIugJ4Grgkbb4CuBDoAl4FLgeIiH2SbgDWpe2+GhE9jfVXkfUAGwU8kCYzM6uh0pJIRFzWx6rzK2wbwPw+jrMIWFQh3gmcfjRlNDOzo+Mn1s3MrDAnETMzK8wDMJrZsOU3FpbPNREzMyvMNREzs2Gk1rUv10TMzKwwJxEzMyvMScTMhhU/lV5bTiJmZlaYk4iZmRXmJGJmZoU5iZiZWWFOImZmVpiTiJmZFeYn1s2soXl8rPpyTcTMzApzEjEzs8KcRMzMrDC3iZhZw/GwJkOHayJmZlZYXZKIpJ2SNknaIKkzxcZKWiVpe/ock+KSdJukLkkbJU3LHacjbb9dUkc9zsXMrJnVsybyRxExNSLa0vIC4MGImAw8mJYBZgGT0zQPuAOypANcB5wDTAeu60k8ZmZWG0PpdtZsYHGaXwzMycWXRGY1MFrSeOACYFVE7IuIF4BVQHuNy2xm1tTqlUQC+AdJ6yXNS7FxEbEnzf8SGJfmJwDP5PbdlWJ9xc1sGPJ7QoamevXO+nBE7Jb0TmCVpMfzKyMiJMVgfVlKVPMATjnllME6rJlZ06tLTSQidqfPvcCPydo0nku3qUife9Pmu4FJud0nplhf8UrftzAi2iKiraWlZTBPxcysqdU8iUg6UdLbeuaBmcBmYDnQ08OqA1iW5pcDc1MvrRnAS+m210pgpqQxqUF9ZoqZ2TDQc/vKt7CGtnrczhoH/FhSz/d/LyJ+ImkdcI+kK4CngUvS9iuAC4Eu4FXgcoCI2CfpBmBd2u6rEbGvdqdhZmY1TyIRsQM4o0L8eeD8CvEA5vdxrEXAosEuo5mZVWcodfE1M7MG4yRiZmaFeQBGMxsy3IjeeFwTMTOzwpxEzMysMN/OMrNBV+17z/1+9MbnJGJmpetJFjtvusjtHsOMk4iZVa13zcHJwdwmYmZmhTmJmFm/PH6V9cdJxGwY6y8BODnYYHCbiFkDyrdFlHXsso5vw4uTiFkDGIwus9WuMxsIJxGzBucEYPXkJGI2RDk5WCNwEjEbItwWYY3IvbPMzKww10TM6si3rKzRuSZiVmN+PsOGE9dEzErQV5JwW4cNN04iZoPAjeLWrBo+iUhqB/4aGAH8XUTcVOciWQMoMhqtR601O1xDt4lIGgF8E5gFTAEukzSlvqWyocptEWaDr9FrItOBrojYASBpKTAb2FrXUhVU5nhIQ1Vf59xXTaG3gdQizGzwKSLqXYbCJF0MtEfEf0rLnwHOiYire203D5iXFt8PPHGEQ58M/GqQi9vIfD0O5etxKF+PQw3X6/EHEdHSO9joNZGqRMRCYGG120vqjIi2EovUUHw9DuXrcShfj0M12/Vo6DYRYDcwKbc8McXMzKwGGj2JrAMmSzpV0nHApcDyOpfJzKxpNPTtrIg4KOlqYCVZF99FEbFlEA5d9a2vJuHrcShfj0P5ehyqqa5HQzesm5lZfTX67SwzM6sjJxEzMyusaZKIpEWS9kranIudIen/Sdok6e8lnZTiH5O0PsXXSzovt89ZKd4l6TZJqsf5HK2BXI/c+lMkvSzpz3KxdklPpOuxoJbnMJgGej0kfSit25LWH5/iDf/7GOB/KyMlLU7xbZKuze0zXH4bkyQ9JGlr+nt/PsXHSlolaXv6HJPiSn/7LkkbJU3LHasjbb9dUke9zmlQRURTTMC/A6YBm3OxdcBH0/xngRvS/JnAu9L86cDu3D5rgRmAgAeAWfU+t7KvR279vcAPgD9LyyOAJ4F3A8cBjwFT6n1uNfh9HAtsBM5Iy+8ARgyX38cAr8WngaVp/gRgJ9A6zH4b44Fpaf5twL+QDbP0dWBBii8Abk7zF6a/vdJvYU2KjwV2pM8xaX5Mvc/vaKemqYlExMPAvl7h9wEPp/lVwCfTto9GxLMpvgUYJektksYDJ0XE6sh+FUuAOaUXvgQDuR4AkuYAT5Fdjx6/H3YmIg4APcPONJwBXo+ZwMaIeCzt+3xEvD5cfh8DvBYBnCjpWGAUcADYz/D6beyJiEfS/K+BbcAEsvNZnDZbzJt/69nAksisBkan38YFwKqI2BcRL5Bdx/banUk5miaJ9GELb/6wP8WhDy72+CTwSES8RvbD2ZVbtyvFhouK10PSW4FrgK/02n4C8ExuuSmuB9k/qCFppaRHJP23FB/Ov4++rsW9wCvAHuAXwF9ExD6G6W9DUivZnYo1wLiI2JNW/RIYl+b7OvdheU2aPYl8FrhK0nqyauqB/EpJHwBuBj5Xh7LVQ1/X43rg1oh4uV4Fq5O+rsexwIeBP0mfn5B0fn2KWDN9XYvpwOvAu4BTgS9Kend9iliu9D9TPwS+EBH78+tSzbMpn5do6IcNj1ZEPE52awJJ7wN+P9SrpInAj4G5EfFkCu8mG1qlx7AaZqWf63EOcLGkrwOjgTck/RZYzzAedqaf67ELeDgifpXWrSBrQ/gOw/T30c+1+DTwk4j4HbBX0j8DbWT/xz1sfhuSRpIlkO9GxI9S+DlJ4yNiT7pdtTfF+xqOaTdwbq/4z8osdy00dU1E0jvT5zHAl4G/TcujgfvJGs3+uWf7VHXdL2lG6nUzF1hW63KXpa/rEREfiYjWiGgF/gr4nxHxDYb5sDN9XQ+yERI+KOmE1BbwUWDrcP599HMtfgGcl9adSNaQ/DjD6LeR/pZ3Atsi4pbcquVATw+rDt78Wy8H5qZeWjOAl9JvYyUwU9KY1JNrZoo1tnq37NdqAr5Pdt/2d2T/J3kF8Hmynhb/AtzEm0/wf5nsPu+G3PTOtK4N2EzW8+QbPfs02jSQ69Frv+tJvbPS8oVp+yeB/17v86rV9QD+lKydYDPw9Vy84X8fA/xv5a1kPfa2kL3H58+H4W/jw2S3qjbm/j24kKxX3oPAduCnwNi0vchelvcksAloyx3rs0BXmi6v97kNxuRhT8zMrLCmvp1lZmZHx0nEzMwKcxIxM7PCnETMzKwwJxEzMyvMScSsZOl5gZ9LmpWLfUrST+pZLrPB4C6+ZjUg6XSy5ynOJBsp4lGgPd4cDWEgxzo2Ig4OchHNCnESMauRNGzMK8CJ6fMPyF41MBK4PiKWpQH+vp22Abg6Iv6vpHOBG4AXgNMi4n21Lb1ZZU4iZjWShgV5hGzwwvuALRHxnTTMzlqyWkoAb0TEbyVNBr4fEW0pidwPnB4RT9Wj/GaVNPUAjGa1FBGvSLobeBm4BPi43nxL5PHAKcCzwDckTSUbHTdf41jrBGJDjZOIWW29kSYBn4yIJ/IrJV0PPAecQdbx5be51a/UqIxmVXPvLLP6WAn8lzRCLJLOTPG3A3si4g3gM2SvmTUbspxEzOrjBrIG9Y2StqRlgNuBDkmPAafh2ocNcW5YNzOzwlwTMTOzwpxEzMysMCcRMzMrzEnEzMwKcxIxM7PCnETMzKwwJxEzMyvs/wNyUwnpXZgoDwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#plot year frequency from train dataset\n",
        "import matplotlib.pyplot as plt\n",
        "train_df = df[:463714]\n",
        "year_counts = train_df.iloc[:, 0].value_counts()\n",
        "plt.bar(year_counts.index, year_counts.values)\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiR5yP7wBxka"
      },
      "source": [
        "##3. Ridge regression\n",
        "\n",
        "* Implement stochastic gradient descent with mini-batches to minimize the loss and evaluate the train and test MSE.\n",
        "* Tune the learning rate and weight decay factor. \n",
        "* Show the train and test loss as a function of epochs, where the number of epochs should be chosen to ensure the train loss is minimized."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_batch_gradient_descent(X_train, y_train, X_test, y_test, batch_size=32, num_epochs=100, learning_rate=0.01, weight_decay_factor=0, loss_type='L2', weight_decay_form='none', momentum=False, momentum_factor=0.9):\n",
        "    num_features = X_train.shape[1]\n",
        "    num_batches = int(np.ceil(len(X_train) / batch_size))\n",
        "    weight = np.random.normal(size=num_features)\n",
        "    m = np.zeros_like(weight)\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    def forward(X, w):\n",
        "        return np.dot(X, w)\n",
        "\n",
        "    def backward(X, error):\n",
        "        return np.dot(X.T, error)\n",
        "\n",
        "    def compute_gradient(X, y, y_pred, loss_type, w):\n",
        "      error = None\n",
        "      if loss_type == \"L2\":\n",
        "          error = 2*(y_pred - y)\n",
        "      elif loss_type == \"count\":\n",
        "          error = np.round(y_pred) - np.round(y)\n",
        "      elif loss_type == \"cross-entropy\":\n",
        "          error = y_pred - y\n",
        "      elif loss_type == 'L1':\n",
        "          error = np.sign(y_pred - y)\n",
        "      gradient = backward(X, error)\n",
        "      if weight_decay_form == 'L2':\n",
        "          gradient += weight_decay_factor * w\n",
        "      elif weight_decay_form == 'L1':\n",
        "          gradient += weight_decay_factor * np.sign(w)\n",
        "      return gradient, error\n",
        "\n",
        "    def compute_loss(y, y_pred, loss_type):\n",
        "        if loss_type == \"L2\":\n",
        "            return np.mean(np.square(y - y_pred))\n",
        "        elif loss_type == \"count\":\n",
        "            return np.mean(np.abs(np.round(y) - np.round(y_pred)))\n",
        "        elif loss_type == \"cross-entropy\":\n",
        "            y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # clip predictions\n",
        "            return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "        elif loss_type == 'L1':\n",
        "            return np.mean(np.abs(y - y_pred))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle the data\n",
        "        perm = np.random.permutation(len(X_train))\n",
        "        X_train = X_train[perm]\n",
        "        y_train = y_train[perm]\n",
        "\n",
        "        # Mini-batch gradient descent\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = (i + 1) * batch_size\n",
        "            X_batch = X_train[start_idx:end_idx]\n",
        "            y_batch = y_train[start_idx:end_idx]\n",
        "\n",
        "            y_pred = forward(X_batch, weight)\n",
        "            gradient, error = compute_gradient(X_batch, y_batch, y_pred, loss_type, weight)\n",
        "\n",
        "            if momentum:\n",
        "              m = momentum_factor * m + (1 - momentum_factor) * gradient\n",
        "              weight -= learning_rate * m\n",
        "            else:\n",
        "                weight -= learning_rate * gradient.reshape(weight.shape)\n",
        "\n",
        "        # Compute train and test losses\n",
        "        y_train_pred = forward(X_train, weight)\n",
        "        train_loss = compute_loss(y_train, y_train_pred, loss_type)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        y_test_pred = forward(X_test, weight)\n",
        "        test_loss = compute_loss(y_test, y_test_pred, loss_type)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        print(\"Epoch:\", epoch+1, \"/100, Train loss:\", train_loss, \"Test loss:\", test_loss)\n",
        "\n",
        "    return weight, train_losses, test_losses\n"
      ],
      "metadata": {
        "id": "f7u12GIuw_jz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning\n",
        "* Learning rate= 0.0001\n",
        "* Weight decay factor= 0.000001\n",
        "* Batch size=16\n",
        "* Number epochs=100\n",
        "* Loss type= L2\n",
        "* Weight decay form= None\n",
        "* No momentum\n",
        "\n"
      ],
      "metadata": {
        "id": "r27YeMIDlVMX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lHLX_tNcbLWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df50472-6513-464e-eae7-4f4270f4195b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 /100, Train loss: 624839.8820521721 Test loss: 625010.5097494762\n",
            "Epoch: 2 /100, Train loss: 97839.30730128195 Test loss: 97906.36282800444\n",
            "Epoch: 3 /100, Train loss: 15387.561570514552 Test loss: 15413.862820671611\n",
            "Epoch: 4 /100, Train loss: 2486.9735997181747 Test loss: 2496.7233110270736\n",
            "Epoch: 5 /100, Train loss: 467.67143205256934 Test loss: 471.04698430176325\n",
            "Epoch: 6 /100, Train loss: 151.25108708362524 Test loss: 152.0418374364628\n",
            "Epoch: 7 /100, Train loss: 101.44475297356688 Test loss: 101.27145340350582\n",
            "Epoch: 8 /100, Train loss: 93.46283563825686 Test loss: 92.92679805853793\n",
            "Epoch: 9 /100, Train loss: 92.05457023456498 Test loss: 91.35498525369832\n",
            "Epoch: 10 /100, Train loss: 91.73512930106122 Test loss: 90.96390578228576\n",
            "Epoch: 11 /100, Train loss: 91.60593162896284 Test loss: 90.83605767000594\n",
            "Epoch: 12 /100, Train loss: 91.53167570769749 Test loss: 90.73624154530387\n",
            "Epoch: 13 /100, Train loss: 91.4779405084195 Test loss: 90.6675222297566\n",
            "Epoch: 14 /100, Train loss: 91.43751737113755 Test loss: 90.62628715351995\n",
            "Epoch: 15 /100, Train loss: 91.40776077514947 Test loss: 90.5813177593688\n",
            "Epoch: 16 /100, Train loss: 91.38084177968365 Test loss: 90.5866090754688\n",
            "Epoch: 17 /100, Train loss: 91.36324275940336 Test loss: 90.61023768884401\n",
            "Epoch: 18 /100, Train loss: 91.34563151657898 Test loss: 90.57118400009996\n",
            "Epoch: 19 /100, Train loss: 91.33348020129208 Test loss: 90.54634687278947\n",
            "Epoch: 20 /100, Train loss: 91.32220380094346 Test loss: 90.5164596988348\n",
            "Epoch: 21 /100, Train loss: 91.31547596380224 Test loss: 90.57442830881563\n",
            "Epoch: 22 /100, Train loss: 91.30625209174785 Test loss: 90.5060828579191\n",
            "Epoch: 23 /100, Train loss: 91.299054209466 Test loss: 90.50029648293823\n",
            "Epoch: 24 /100, Train loss: 91.2929631848196 Test loss: 90.49216302826795\n",
            "Epoch: 25 /100, Train loss: 91.28858538071866 Test loss: 90.52721202515598\n",
            "Epoch: 26 /100, Train loss: 91.28584079710944 Test loss: 90.47599969843617\n",
            "Epoch: 27 /100, Train loss: 91.28287184392332 Test loss: 90.49694780689998\n",
            "Epoch: 28 /100, Train loss: 91.27864131084205 Test loss: 90.51058755612944\n",
            "Epoch: 29 /100, Train loss: 91.27549487468335 Test loss: 90.50317983983759\n",
            "Epoch: 30 /100, Train loss: 91.27270693964361 Test loss: 90.51035559796804\n",
            "Epoch: 31 /100, Train loss: 91.27277120194984 Test loss: 90.5149751862915\n",
            "Epoch: 32 /100, Train loss: 91.27000320464086 Test loss: 90.50652316210618\n",
            "Epoch: 33 /100, Train loss: 91.27124602458889 Test loss: 90.52306960113461\n",
            "Epoch: 34 /100, Train loss: 91.2701783480846 Test loss: 90.45270518272105\n",
            "Epoch: 35 /100, Train loss: 91.2689420598024 Test loss: 90.46177490282345\n",
            "Epoch: 36 /100, Train loss: 91.26534407817476 Test loss: 90.4961031324311\n",
            "Epoch: 37 /100, Train loss: 91.26329453150872 Test loss: 90.48998932385236\n",
            "Epoch: 38 /100, Train loss: 91.26404334725119 Test loss: 90.51381189994508\n",
            "Epoch: 39 /100, Train loss: 91.26605988586844 Test loss: 90.4709271221983\n",
            "Epoch: 40 /100, Train loss: 91.26215694739541 Test loss: 90.48201652630856\n",
            "Epoch: 41 /100, Train loss: 91.26528312937576 Test loss: 90.52480298477333\n",
            "Epoch: 42 /100, Train loss: 91.26141238327958 Test loss: 90.5028399032914\n",
            "Epoch: 43 /100, Train loss: 91.26089996616318 Test loss: 90.5044036940258\n",
            "Epoch: 44 /100, Train loss: 91.26176568605753 Test loss: 90.47750473945302\n",
            "Epoch: 45 /100, Train loss: 91.26519784066812 Test loss: 90.49001243608372\n",
            "Epoch: 46 /100, Train loss: 91.26484112104387 Test loss: 90.53050771097989\n",
            "Epoch: 47 /100, Train loss: 91.25975189116399 Test loss: 90.49151598977039\n",
            "Epoch: 48 /100, Train loss: 91.25999060328878 Test loss: 90.50294359112516\n",
            "Epoch: 49 /100, Train loss: 91.26258885823478 Test loss: 90.52623534242466\n",
            "Epoch: 50 /100, Train loss: 91.2597134465397 Test loss: 90.48602007999968\n",
            "Epoch: 51 /100, Train loss: 91.26001764176797 Test loss: 90.49853651022902\n",
            "Epoch: 52 /100, Train loss: 91.25926708351206 Test loss: 90.50570150951546\n",
            "Epoch: 53 /100, Train loss: 91.26100906024239 Test loss: 90.51446435374017\n",
            "Epoch: 54 /100, Train loss: 91.26037992587061 Test loss: 90.4911973240174\n",
            "Epoch: 55 /100, Train loss: 91.26282202674363 Test loss: 90.51840700530512\n",
            "Epoch: 56 /100, Train loss: 91.25773572104622 Test loss: 90.4953405070494\n",
            "Epoch: 57 /100, Train loss: 91.25768496964207 Test loss: 90.48963331421272\n",
            "Epoch: 58 /100, Train loss: 91.25953775125433 Test loss: 90.50176899947763\n",
            "Epoch: 59 /100, Train loss: 91.26876663620624 Test loss: 90.47110218311069\n",
            "Epoch: 60 /100, Train loss: 91.25878502652088 Test loss: 90.50202789903646\n",
            "Epoch: 61 /100, Train loss: 91.25996027057596 Test loss: 90.51307801742539\n",
            "Epoch: 62 /100, Train loss: 91.25891115974159 Test loss: 90.51145385689088\n",
            "Epoch: 63 /100, Train loss: 91.26062278334336 Test loss: 90.46685793302652\n",
            "Epoch: 64 /100, Train loss: 91.25938957066327 Test loss: 90.49420398098212\n",
            "Epoch: 65 /100, Train loss: 91.25923415394831 Test loss: 90.47409754275998\n",
            "Epoch: 66 /100, Train loss: 91.25982469981963 Test loss: 90.51455065628646\n",
            "Epoch: 67 /100, Train loss: 91.25768862799387 Test loss: 90.49138865491594\n",
            "Epoch: 68 /100, Train loss: 91.26202266523613 Test loss: 90.5265982592084\n",
            "Epoch: 69 /100, Train loss: 91.25993132416596 Test loss: 90.5033400720069\n",
            "Epoch: 70 /100, Train loss: 91.2619841012415 Test loss: 90.52937672156278\n",
            "Epoch: 71 /100, Train loss: 91.2582934993155 Test loss: 90.50614022342808\n",
            "Epoch: 72 /100, Train loss: 91.25946452888576 Test loss: 90.5108275482185\n",
            "Epoch: 73 /100, Train loss: 91.25734291898395 Test loss: 90.50345206643813\n",
            "Epoch: 74 /100, Train loss: 91.26143550752762 Test loss: 90.52944751070997\n",
            "Epoch: 75 /100, Train loss: 91.26106337226261 Test loss: 90.4894511023746\n",
            "Epoch: 76 /100, Train loss: 91.25925361735915 Test loss: 90.47021550771596\n",
            "Epoch: 77 /100, Train loss: 91.25945350718754 Test loss: 90.48384989273309\n",
            "Epoch: 78 /100, Train loss: 91.25852332079751 Test loss: 90.48043438824678\n",
            "Epoch: 79 /100, Train loss: 91.26015863865187 Test loss: 90.4758453781572\n",
            "Epoch: 80 /100, Train loss: 91.25876762036101 Test loss: 90.47434195195757\n",
            "Epoch: 81 /100, Train loss: 91.25907198737974 Test loss: 90.47309969341912\n",
            "Epoch: 82 /100, Train loss: 91.25828235881656 Test loss: 90.48007843324186\n",
            "Epoch: 83 /100, Train loss: 91.26374490167373 Test loss: 90.54546258578166\n",
            "Epoch: 84 /100, Train loss: 91.25730569818386 Test loss: 90.494455930384\n",
            "Epoch: 85 /100, Train loss: 91.26083870148462 Test loss: 90.52640119048444\n",
            "Epoch: 86 /100, Train loss: 91.25898900594365 Test loss: 90.52503488305058\n",
            "Epoch: 87 /100, Train loss: 91.26128450333361 Test loss: 90.51149449179835\n",
            "Epoch: 88 /100, Train loss: 91.2579967745125 Test loss: 90.49029424323282\n",
            "Epoch: 89 /100, Train loss: 91.25861229156374 Test loss: 90.49702974956728\n",
            "Epoch: 90 /100, Train loss: 91.26031415303396 Test loss: 90.47353511951076\n",
            "Epoch: 91 /100, Train loss: 91.25890082936151 Test loss: 90.49194214309036\n",
            "Epoch: 92 /100, Train loss: 91.25767217850259 Test loss: 90.48340120309149\n",
            "Epoch: 93 /100, Train loss: 91.2584787532369 Test loss: 90.49216581230955\n",
            "Epoch: 94 /100, Train loss: 91.26136483599139 Test loss: 90.53634757577119\n",
            "Epoch: 95 /100, Train loss: 91.25761465921381 Test loss: 90.50548656175205\n",
            "Epoch: 96 /100, Train loss: 91.26435109616774 Test loss: 90.53529968214261\n",
            "Epoch: 97 /100, Train loss: 91.26017007002906 Test loss: 90.50473746807856\n",
            "Epoch: 98 /100, Train loss: 91.2601590146335 Test loss: 90.50010489017099\n",
            "Epoch: 99 /100, Train loss: 91.25936210528648 Test loss: 90.49805419053178\n",
            "Epoch: 100 /100, Train loss: 91.26011248858828 Test loss: 90.47206004733344\n"
          ]
        }
      ],
      "source": [
        "w_ridge, train_loss_ridge, test_loss_ridge = mini_batch_gradient_descent(train_x, train_y, test_x, test_y, batch_size=16, num_epochs=100, learning_rate=0.000001, weight_decay_factor=0.000001, loss_type='L2', weight_decay_form=None, momentum=False, momentum_factor=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "a7olCs7GbNW5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "93c9a152-1f66-45b1-9851-186869fbcc37"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "num_epochs=100\n",
        "plt.plot(range(num_epochs), train_loss_ridge,'r', label=\"Train loss\")\n",
        "plt.plot(range(num_epochs), test_loss_ridge, 'bo',label=\"Test loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9ap9a_o1OGE"
      },
      "source": [
        "Pseudoinverse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(X, y, weight, loss_type):\n",
        "    if loss_type == 'L2':\n",
        "        loss = np.mean((np.dot(X, weight) - y) ** 2)\n",
        "    elif loss_type == 'count':\n",
        "        loss = np.mean(np.abs(np.dot(X, weight) - y))\n",
        "    elif loss_type == 'cross-entropy':\n",
        "        exp_term = np.exp(np.dot(X, weight))\n",
        "        loss = np.mean(np.log(1 + exp_term) - y * np.dot(X, weight))\n",
        "    return loss\n",
        "\n",
        "def pseudoinverse(train_x, train_y, test_x, test_y, alpha=0, loss_type='L2'):\n",
        "    pseudoinv= np.dot(np.linalg.inv(train_x.T.dot(train_x) + alpha*np.eye(train_x.shape[1])), train_x.T)\n",
        "    weight= np.dot(pseudoinv, train_y)\n",
        "    train_y_predict= np.dot(train_x, weight)\n",
        "    test_y_predict= np.dot(test_x, weight)\n",
        "\n",
        "    train_loss= compute_loss(train_x, train_y, weight, loss_type=loss_type)\n",
        "    test_loss= compute_loss(test_x, test_y,weight, loss_type=loss_type)\n",
        "  \n",
        "    print(f'Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}')\n",
        "    \n",
        "    return weight, train_loss, test_loss"
      ],
      "metadata": {
        "id": "ohY45Ph3pYrd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_inverse, train_loss_inverse, test_loss_inverse = pseudoinverse(train_x, train_y, test_x, test_y, alpha=0, loss_type='L2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUVECvJdDX7I",
        "outputId": "f514d1e1-89e5-4ae5-c1a2-b447afc5b99e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 91.2564, Test loss: 90.4911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Implement L1 weight decay"
      ],
      "metadata": {
        "id": "jL8oRaDeDY-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_lasso, train_loss_lasso, test_loss_lasso = mini_batch_gradient_descent(train_x, train_y, test_x, test_y, batch_size=32, num_epochs=100, learning_rate=0.0001, weight_decay_factor=0.1, loss_type='L1', weight_decay_form='L1', momentum=False, momentum_factor=None)\n",
        "num_epochs=100\n",
        "plt.plot(range(num_epochs), train_loss_lasso,'r', label=\"Train loss\")\n",
        "plt.plot(range(num_epochs), test_loss_lasso, 'bo',label=\"Test loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "erfbyKAdDhYL",
        "outputId": "a7cd58ef-8942-40ee-b64e-d8b8137e0606"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 /100, Train loss: 1952.2429313831774 Test loss: 1952.3527780436468\n",
            "Epoch: 2 /100, Train loss: 1906.016451383165 Test loss: 1906.1262980436347\n",
            "Epoch: 3 /100, Train loss: 1859.7899713831607 Test loss: 1859.89981804363\n",
            "Epoch: 4 /100, Train loss: 1813.5634913833146 Test loss: 1813.673338043784\n",
            "Epoch: 5 /100, Train loss: 1767.3370113834692 Test loss: 1767.446858043938\n",
            "Epoch: 6 /100, Train loss: 1721.1105313834337 Test loss: 1721.2203780439027\n",
            "Epoch: 7 /100, Train loss: 1674.8840513831758 Test loss: 1674.9938980436448\n",
            "Epoch: 8 /100, Train loss: 1628.657571382918 Test loss: 1628.7674180433874\n",
            "Epoch: 9 /100, Train loss: 1582.43109138266 Test loss: 1582.5409380431292\n",
            "Epoch: 10 /100, Train loss: 1536.2046113824026 Test loss: 1536.3144580428718\n",
            "Epoch: 11 /100, Train loss: 1489.9781313821445 Test loss: 1490.0879780426137\n",
            "Epoch: 12 /100, Train loss: 1443.7516513818869 Test loss: 1443.861498042356\n",
            "Epoch: 13 /100, Train loss: 1397.5251713816294 Test loss: 1397.6350180420984\n",
            "Epoch: 14 /100, Train loss: 1351.2986913813716 Test loss: 1351.4085380418408\n",
            "Epoch: 15 /100, Train loss: 1305.0722113811144 Test loss: 1305.1820580415826\n",
            "Epoch: 16 /100, Train loss: 1258.845731380856 Test loss: 1258.955578041325\n",
            "Epoch: 17 /100, Train loss: 1212.619251380598 Test loss: 1212.7290980410671\n",
            "Epoch: 18 /100, Train loss: 1166.39277138034 Test loss: 1166.5026180408095\n",
            "Epoch: 19 /100, Train loss: 1120.1662913800826 Test loss: 1120.2761380405516\n",
            "Epoch: 20 /100, Train loss: 1073.9398113798252 Test loss: 1074.049658040294\n",
            "Epoch: 21 /100, Train loss: 1027.713331379567 Test loss: 1027.823178040036\n",
            "Epoch: 22 /100, Train loss: 981.4868513793097 Test loss: 981.5966980397783\n",
            "Epoch: 23 /100, Train loss: 935.2603713804461 Test loss: 935.370218040915\n",
            "Epoch: 24 /100, Train loss: 889.0338913818356 Test loss: 889.1437380423048\n",
            "Epoch: 25 /100, Train loss: 842.8074113832251 Test loss: 842.9172580436943\n",
            "Epoch: 26 /100, Train loss: 796.5809313846152 Test loss: 796.690778045084\n",
            "Epoch: 27 /100, Train loss: 750.3544513860048 Test loss: 750.4642980464737\n",
            "Epoch: 28 /100, Train loss: 704.1279713873943 Test loss: 704.2378180478634\n",
            "Epoch: 29 /100, Train loss: 657.9014913887839 Test loss: 658.011338049253\n",
            "Epoch: 30 /100, Train loss: 611.6750113901736 Test loss: 611.7848580506427\n",
            "Epoch: 31 /100, Train loss: 565.4485313915635 Test loss: 565.5583780520324\n",
            "Epoch: 32 /100, Train loss: 519.2220513929531 Test loss: 519.3318980534219\n",
            "Epoch: 33 /100, Train loss: 472.99557139434256 Test loss: 473.1054180548117\n",
            "Epoch: 34 /100, Train loss: 426.7690913957324 Test loss: 426.8789380562014\n",
            "Epoch: 35 /100, Train loss: 380.5426113971221 Test loss: 380.652458057591\n",
            "Epoch: 36 /100, Train loss: 334.31613139851163 Test loss: 334.42597805898066\n",
            "Epoch: 37 /100, Train loss: 288.08965139990147 Test loss: 288.19949806037033\n",
            "Epoch: 38 /100, Train loss: 241.8631714012911 Test loss: 241.97301806176003\n",
            "Epoch: 39 /100, Train loss: 195.63669140268075 Test loss: 195.7465380631497\n",
            "Epoch: 40 /100, Train loss: 149.41021140407037 Test loss: 149.5200580645394\n",
            "Epoch: 41 /100, Train loss: 103.18373140546007 Test loss: 103.29357806592905\n",
            "Epoch: 42 /100, Train loss: 56.99587590974257 Test loss: 57.10205796874035\n",
            "Epoch: 43 /100, Train loss: 15.42789255441484 Test loss: 15.47537573719086\n",
            "Epoch: 44 /100, Train loss: 6.589358333560247 Test loss: 6.598300323428438\n",
            "Epoch: 45 /100, Train loss: 6.530413169250414 Test loss: 6.544228522013428\n",
            "Epoch: 46 /100, Train loss: 6.5198714746632 Test loss: 6.534173069917074\n",
            "Epoch: 47 /100, Train loss: 6.5184344648549075 Test loss: 6.536502374044339\n",
            "Epoch: 48 /100, Train loss: 6.5175142664298935 Test loss: 6.527642612275455\n",
            "Epoch: 49 /100, Train loss: 6.516707552465168 Test loss: 6.5293393638222375\n",
            "Epoch: 50 /100, Train loss: 6.515599047530417 Test loss: 6.530069261960776\n",
            "Epoch: 51 /100, Train loss: 6.517658880112071 Test loss: 6.534745039473232\n",
            "Epoch: 52 /100, Train loss: 6.516099250629805 Test loss: 6.53116242126082\n",
            "Epoch: 53 /100, Train loss: 6.5160829045748425 Test loss: 6.5299107531060505\n",
            "Epoch: 54 /100, Train loss: 6.51630386110147 Test loss: 6.527743800986595\n",
            "Epoch: 55 /100, Train loss: 6.515547619021382 Test loss: 6.531618897951545\n",
            "Epoch: 56 /100, Train loss: 6.515956902214356 Test loss: 6.530403467671542\n",
            "Epoch: 57 /100, Train loss: 6.516019509017972 Test loss: 6.531168284325177\n",
            "Epoch: 58 /100, Train loss: 6.516205619447756 Test loss: 6.5279637010508225\n",
            "Epoch: 59 /100, Train loss: 6.515747018852855 Test loss: 6.529548031575573\n",
            "Epoch: 60 /100, Train loss: 6.516025984654695 Test loss: 6.530550637305797\n",
            "Epoch: 61 /100, Train loss: 6.516722007697826 Test loss: 6.533292842497669\n",
            "Epoch: 62 /100, Train loss: 6.516886242769689 Test loss: 6.532901973554972\n",
            "Epoch: 63 /100, Train loss: 6.515835350476184 Test loss: 6.52801589316877\n",
            "Epoch: 64 /100, Train loss: 6.5163567948382 Test loss: 6.531325614116581\n",
            "Epoch: 65 /100, Train loss: 6.516199442456442 Test loss: 6.527781202345015\n",
            "Epoch: 66 /100, Train loss: 6.515821794056767 Test loss: 6.528735950720291\n",
            "Epoch: 67 /100, Train loss: 6.516187731521043 Test loss: 6.531468214218161\n",
            "Epoch: 68 /100, Train loss: 6.5164958282889005 Test loss: 6.529309102452502\n",
            "Epoch: 69 /100, Train loss: 6.515813594205168 Test loss: 6.530171138980544\n",
            "Epoch: 70 /100, Train loss: 6.515346698086049 Test loss: 6.528112080958623\n",
            "Epoch: 71 /100, Train loss: 6.515971399091788 Test loss: 6.527830963160529\n",
            "Epoch: 72 /100, Train loss: 6.515833257167878 Test loss: 6.529347855665879\n",
            "Epoch: 73 /100, Train loss: 6.516385620875133 Test loss: 6.533324954711613\n",
            "Epoch: 74 /100, Train loss: 6.51562380255872 Test loss: 6.530386996369317\n",
            "Epoch: 75 /100, Train loss: 6.515163798233277 Test loss: 6.5275896743939015\n",
            "Epoch: 76 /100, Train loss: 6.516114177983203 Test loss: 6.528630358565727\n",
            "Epoch: 77 /100, Train loss: 6.515000780934302 Test loss: 6.528998587180315\n",
            "Epoch: 78 /100, Train loss: 6.516845146955883 Test loss: 6.532380085515584\n",
            "Epoch: 79 /100, Train loss: 6.517889328668159 Test loss: 6.525012195863966\n",
            "Epoch: 80 /100, Train loss: 6.517167627076448 Test loss: 6.536046850848777\n",
            "Epoch: 81 /100, Train loss: 6.517002224075953 Test loss: 6.531609338627126\n",
            "Epoch: 82 /100, Train loss: 6.518059762718259 Test loss: 6.526223784871655\n",
            "Epoch: 83 /100, Train loss: 6.515888620675267 Test loss: 6.5263308734256515\n",
            "Epoch: 84 /100, Train loss: 6.517235850795844 Test loss: 6.532049837377277\n",
            "Epoch: 85 /100, Train loss: 6.515577799625985 Test loss: 6.528324579227124\n",
            "Epoch: 86 /100, Train loss: 6.51734376091644 Test loss: 6.527130500560758\n",
            "Epoch: 87 /100, Train loss: 6.515360038718288 Test loss: 6.528375145815926\n",
            "Epoch: 88 /100, Train loss: 6.515871354093494 Test loss: 6.52847110137146\n",
            "Epoch: 89 /100, Train loss: 6.516788788025871 Test loss: 6.530641245265426\n",
            "Epoch: 90 /100, Train loss: 6.515590704336841 Test loss: 6.52810446231346\n",
            "Epoch: 91 /100, Train loss: 6.515439859444883 Test loss: 6.52996354664603\n",
            "Epoch: 92 /100, Train loss: 6.515629476064173 Test loss: 6.530566418300115\n",
            "Epoch: 93 /100, Train loss: 6.515953154872637 Test loss: 6.526918306513928\n",
            "Epoch: 94 /100, Train loss: 6.515691229155771 Test loss: 6.528193431871071\n",
            "Epoch: 95 /100, Train loss: 6.515722554369684 Test loss: 6.5297053773915765\n",
            "Epoch: 96 /100, Train loss: 6.51580655002265 Test loss: 6.527344945091425\n",
            "Epoch: 97 /100, Train loss: 6.5152513203263505 Test loss: 6.527512202537641\n",
            "Epoch: 98 /100, Train loss: 6.516076343267791 Test loss: 6.530180019469609\n",
            "Epoch: 99 /100, Train loss: 6.515405523192904 Test loss: 6.529644014037986\n",
            "Epoch: 100 /100, Train loss: 6.515748226079772 Test loss: 6.530989262384126\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Count Regression\n",
        "\\begin{align*}\n",
        "y_{pred} &= e^{w^T X}\\\\\n",
        "L(y, y_{pred}) &= e^{-y_{pred}} y_{pred}^y / y!\\\\\n",
        "&= -\\sum_{i=1}^{n}(y_i log(y_{pred,i}) - y_{pred,i})\\\\\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "nCdeXttIDv23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_regression(X_train, y_train, X_test, y_test, batch_size=16, num_epochs=100, learning_rate=0.01, weight_decay_factor=0, loss_type='count', weight_decay_form='none', momentum=False, momentum_factor=0.9):\n",
        "    num_features = X_train.shape[1]\n",
        "    num_batches = int(np.ceil(len(X_train) / batch_size))\n",
        "    weight = np.random.normal(size=num_features)\n",
        "    m = 0\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    def forward(X, w):\n",
        "      # Clip X * w to avoid overflow in the exponential function\n",
        "      Xw = np.clip(np.dot(X, w), -100, None)\n",
        "      # Apply the exponential function and replace any resulting inf or nan values\n",
        "      y_pred = np.where(np.isinf(np.exp(Xw)), 1e10, np.exp(np.clip(Xw, -500, 500)))\n",
        "      return y_pred\n",
        "\n",
        "    def backward(X, error):\n",
        "        return np.dot(X.T, error)\n",
        "\n",
        "    def compute_gradient(X, y, y_pred, loss_type, w):\n",
        "      error = None\n",
        "      if loss_type == \"L2\":\n",
        "          error = 2*(y_pred - y)\n",
        "      elif loss_type == \"count\":\n",
        "          # Clip y_pred to avoid overflow\n",
        "          y_pred_clipped = np.clip(y_pred, -100, None)\n",
        "          error = np.exp(-y_pred_clipped) * (y_pred - y)\n",
        "      elif loss_type == \"cross-entropy\":\n",
        "          error = y_pred - y\n",
        "\n",
        "      gradient = backward(X, error)\n",
        "      if weight_decay_form == 'L2':\n",
        "          gradient += weight_decay_factor * w\n",
        "      elif weight_decay_form == 'L1':\n",
        "          gradient += weight_decay_factor * np.sign(w)\n",
        "\n",
        "      return gradient, error\n",
        "\n",
        "\n",
        "    def compute_loss(y, y_pred, loss_type):\n",
        "      if loss_type == \"L2\":\n",
        "          return np.mean(np.square(y - y_pred))\n",
        "      elif loss_type == \"count\":\n",
        "          # Clip y_pred to avoid overflow\n",
        "          y_pred_clipped = np.clip(y_pred, -100, None)\n",
        "          return np.mean(np.exp(-y_pred_clipped) * np.square(y - y_pred))\n",
        "      elif loss_type == \"cross-entropy\":\n",
        "          y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # clip predictions\n",
        "          return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "      elif loss_type == 'L1':\n",
        "          return np.mean(np.abs(y - y_pred))\n",
        "      else:\n",
        "          raise ValueError(\"Invalid loss type: {}\".format(loss_type))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle the data\n",
        "        perm = np.random.permutation(len(X_train))\n",
        "        X_train = X_train[perm]\n",
        "        y_train = y_train[perm]\n",
        "\n",
        "        # Mini-batch gradient descent\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = (i + 1) * batch_size\n",
        "            X_batch = X_train[start_idx:end_idx]\n",
        "            y_batch = y_train[start_idx:end_idx]\n",
        "\n",
        "            y_pred = forward(X_batch, weight)\n",
        "            gradient, error = compute_gradient(X_batch, y_batch, y_pred, loss_type, weight)\n",
        "\n",
        "            if momentum:\n",
        "                m = momentum_factor * m + (1 - momentum_factor) * gradient\n",
        "                weight -= learning_rate * m\n",
        "            else:\n",
        "                weight -= learning_rate * gradient\n",
        "\n",
        "        # Compute train and test losses\n",
        "        y_train_pred = forward(X_train, weight)\n",
        "        train_loss = compute_loss(y_train, y_train_pred, loss_type)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        y_test_pred = forward(X_test, weight)\n",
        "        test_loss = compute_loss(y_test, y_test_pred, loss_type)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        print(\"Epoch:\", epoch+1, \"/100, Train loss:\", train_loss, \"Test loss:\", test_loss)\n",
        "\n",
        "        # Check if the test loss has stopped decreasing\n",
        "        if len(test_losses) >= 2 and test_losses[-1] >= test_losses[-2]:\n",
        "            print(\"Test loss has stopped decreasing. Stopping training.\")\n",
        "            break\n",
        "\n",
        "    return weight, train_losses, test_losses"
      ],
      "metadata": {
        "id": "cNIIYX9ZDrWg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w, train_loss, test_loss= count_regression(train_x, train_y, test_x, test_y, batch_size=16, num_epochs=100, learning_rate=0.0001, weight_decay_factor=0.0001, loss_type='L2', weight_decay_form='L1', momentum=False, momentum_factor=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxuLKLGkG1m3",
        "outputId": "3f90e7f0-f76c-435b-c5af-ac84ca4fa7c5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-4fd2562a219d>:13: RuntimeWarning: overflow encountered in exp\n",
            "  y_pred = np.where(np.isinf(np.exp(Xw)), 1e10, np.exp(np.clip(Xw, -500, 500)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 /100, Train loss: 4.041002106483271e+19 Test loss: 4.069259088288288e+19\n",
            "Epoch: 2 /100, Train loss: 4.0410021064832705e+19 Test loss: 4.069259088288288e+19\n",
            "Test loss has stopped decreasing. Stopping training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=100\n",
        "plt.plot(range(num_epochs), train_loss,'r', label=\"Train loss\")\n",
        "plt.plot(range(num_epochs), test_loss, 'bo',label=\"Test loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "zs36ofwaG9q_",
        "outputId": "609e484b-40f7-43ca-aa88-48eddac43a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Model comparison\n"
      ],
      "metadata": {
        "id": "m0zHKAuX2wv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fig, ax = plt.subplots(figsize=(8, 6))\n",
        "plt.hist(w_ridge, alpha=0.5, label='Ridge')\n",
        "plt.xlabel('Weight Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Weights - Ridge')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "iYbX4Db7jqkq",
        "outputId": "08e6653e-b37a-4194-8deb-390417fb2dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(w_lasso, alpha=0.5, label='Lasso')\n",
        "plt.xlabel('Weight Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Weights - Lasso')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "kyqJJoVMEXKx",
        "outputId": "9970c72a-9a12-4080-9952-9b8bb4235d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAac0lEQVR4nO3de7xcZX3v8c+XBAgQIGB2U64mIBcjiMRAAxRaBQURCLYchIM1ejhyPF6B8oKIHqQ9RytWSdPaUhCogMhdBa2KgYNEboEEghACJCQgl5BswdwAgcCvf6xnZGUye++1k71msvN836/XvPZaz8ya9ZtnZn9nzTMzzygiMDOzfGzU6QLMzKy9HPxmZplx8JuZZcbBb2aWGQe/mVlmHPxmZplx8BsAkuZI+stO19FJkj4i6WlJKyXtW+N+zpZ0ccXLnivp+3XVYnly8GdA0pOSDmtq+4SkOxrrEfGuiPhVH9czWlJIGlpTqZ32LeBzETE8Ih4onyHpQkkXlNY3lvRSD20TettJRHw9Iv7nQBTc6r4dKJK+J+n/1XHd1lkOfltvrAdPKG8H5vRw3nTgkNL6eOC3wMFNbQCzBr40s4Hj4Ddg9SNHSftLmilpuaTFks5PF5ue/i5NwyEHSNpI0lckPSVpiaTLJW1dut6Pp/NekPR/mvZzrqTrJX1f0nLgE2nfd0taKmmRpO9I2qR0fSHpM5LmSVoh6f9K2lXSXanea8uXb7qNLWuVtKmklcAQ4EFJT7TYfDrwTkkj0/rBwNXAFk1td0fE65K2l3SDpG5JCyV9oVTHasM3vfVRskmqdUUakhuftrsC2Bn4Sbo/zpQ0LPXnC6kP75M0qpe7fq1ImpqGxZZLmiXp4NJ5LR8/vdWW+usmSS9Kmi/pUwNds73FwW+tTAWmRsRWwK7Atam9ccQ7Ig2H3A18Ip3eB+wCDAe+AyBpLPBvwEnAdsDWwA5N+5oIXA+MAK4E3gBOA0YCBwCHAp9p2uZw4L3ABOBM4CLgY8BOwF7AiT3crpa1RsSrETE8XWafiNi1ecOIeBp4ireO8A8Bfg3c1dQ2XdJGwE+AB9PtPRQ4VdLhzddbsY+OoXiSGQHcROrfiPgbilcdR6f745vApHQdOwFvAz4NvNJDf6yL+4D3ANsCPwCukzQsndfT46e32q4GngG2B44Dvi7p/TXUbTj4c/LjdJS1VNJSirDpyevAOySNjIiVEXFPL5c9CTg/IhZExErgS8AJadjmOOAnEXFHRLwGnAM0Tw51d0T8OCLejIhXImJWRNwTEasi4kngQuAvmrb5ZkQsj4g5wMPAL9P+lwE/B3p6Y7a3Wqu4HTgkBfv+wD0U4d9oOyhdZj+gKyL+PiJei4gFwHeBE1pcZ5U+uiMifhYRbwBXAPv0UuPrFKH6joh4I/Xn8oq3r7KI+H5EvJDup28DmwJ7lGpo9fhpWZuknSj67qyI+ENEzAYuBj4+0HVbwcGfj2MjYkTjxJpH0WUnA7sDj6aX40f1ctntKY6EG54ChgKj0nlPN86IiJeBF5q2f7q8Iml3ST+V9Hwa/vk6xdF/2eLS8ist1ofTWm+1VtEY598bWJBuzx2lts2AGRTvFWzf9ER7dg/7qdJHz5eWXwaG9fJkdQVwM3C1pOckfVPSxs0XknRSGh5aKennfd3wFtufIWmupGXp9m3NW/dTT4+fnmrbHngxIlaUdvEUa77ysQHi4Lc1RMS8iDgR+BPgPOB6SVuw5pEowHMUQdewM7CKIowXATs2zpC0GcUR32q7a1q/AHgU2C0NFZwNaO1vTeVaq5hOcbT9YYojfSjeDN4ptd0XEX+gCPKF5SfaiNgyIo5scZ1V+qg3q/VfRLweEX8XEWOBA4GjaHHkHBFXpuGh4RHxoX7sjzSefyZwPLBNOpBYRrqfenr89FLbc8C2krYs7WZn4Nn+1GXVOfhtDZI+JqkrIt4ElqbmN4Hu9HeX0sWvAk6TNEbScIoj9GsiYhXF2P3Rkg5Mb7ieS98hviWwHFgpaU/gfw/Qzeqr1j5FxHyKJ4kvkoI/innNZ6S2xpvf9wIrJJ0laTNJQyTtJWm/Fle7Nn1UtpjS/SHpfZL2ljSEoh9fp7jP1taQ9KZs47QJxX20iuLxMFTSOcBWpRpaPn56qi29f3IX8A9pH++meNXg7y/UxMFvrRwBzFHxSZepwAlp/P1l4GvAnWkIYwJwKcVL+OnAQuAPwOcB0hj85yneuFsErASWAK/2su8zgP8OrKAYF79mAG9Xj7X2w3SgC7iz1PZriqPb6QBpLP4oijc/FwK/oxiz3poma9lHZf8AfCXdH2cAf0rxZLIcmEvxnsMV/bmBTSZTDJ81Tv+fYrjmF8DjFEMyjVc5DS0fP33UdiIwmuLo/0fAVyPilnWo23oh/xCLtUs6yl5KMYyzsMPlrJfcR9YOPuK3Wkk6WtLm6T2CbwEPAU92tqr1i/vI2s3Bb3WbSPHy/TlgN4qX/X6ZuTr3kbWVh3rMzDLjI34zs8x0elKsSkaOHBmjR4/udBlmZoPKrFmzfhcRXc3tgyL4R48ezcyZMztdhpnZoCLpqVbtHuoxM8uMg9/MLDMOfjOzzDj4zcwy4+A3M8uMg9/MLDMOfjOzzDj4zcwy4+A3M8vMoPjm7rqYMu3xjuz3tA/s3pH9mpn1xUf8ZmaZcfCbmWXGwW9mlhkHv5lZZhz8ZmaZcfCbmWXGwW9mlhkHv5lZZhz8ZmaZcfCbmWXGwW9mlhkHv5lZZhz8ZmaZcfCbmWXGwW9mlhkHv5lZZhz8ZmaZcfCbmWXGwW9mlplag1/SaZLmSHpY0lWShkkaI2mGpPmSrpG0SZ01mJnZ6moLfkk7AF8AxkfEXsAQ4ATgPGBKRLwD+D1wcl01mJnZmuoe6hkKbCZpKLA5sAh4P3B9Ov8y4NiaazAzs5Lagj8ingW+BfyWIvCXAbOApRGxKl3sGWCHVttLOkXSTEkzu7u76yrTzCw7dQ71bANMBMYA2wNbAEdU3T4iLoqI8RExvqurq6YqzczyU+dQz2HAwojojojXgR8CBwEj0tAPwI7AszXWYGZmTeoM/t8CEyRtLknAocAjwG3Acekyk4Aba6zBzMya1DnGP4PiTdz7gYfSvi4CzgJOlzQfeBtwSV01mJnZmob2fZG1FxFfBb7a1LwA2L/O/ZqZWc/8zV0zs8w4+M3MMuPgNzPLjIPfzCwzDn4zs8w4+M3MMuPgNzPLjIPfzCwzDn4zs8w4+M3MMuPgNzPLjIPfzCwzDn4zs8w4+M3MMuPgNzPLjIPfzCwzDn4zs8w4+M3MMuPgNzPLjIPfzCwzDn4zs8w4+M3MMuPgNzPLjIPfzCwzDn4zs8w4+M3MMuPgNzPLjIPfzCwzDn4zs8w4+M3MMuPgNzPLjIPfzCwzDn4zs8w4+M3MMuPgNzPLjIPfzCwzDn4zs8zUGvySRki6XtKjkuZKOkDStpKmSZqX/m5TZw1mZra6uo/4pwK/iIg9gX2AucBk4NaI2A24Na2bmVmb1Bb8krYGDgEuAYiI1yJiKTARuCxd7DLg2LpqMDOzNdV5xD8G6Ab+Q9IDki6WtAUwKiIWpcs8D4xqtbGkUyTNlDSzu7u7xjLNzPJSZ/APBcYBF0TEvsBLNA3rREQA0WrjiLgoIsZHxPiurq4ayzQzy0udwf8M8ExEzEjr11M8ESyWtB1A+rukxhrMzKxJbcEfEc8DT0vaIzUdCjwC3ARMSm2TgBvrqsHMzNY0tObr/zxwpaRNgAXAJymebK6VdDLwFHB8zTWYmVlJrcEfEbOB8S3OOrTO/ZqZWc8qDfVI2rvuQszMrD2qjvH/m6R7JX0mfT7fzMwGqUrBHxEHAycBOwGzJP1A0gdqrczMzGpR+VM9ETEP+ApwFvAXwD+nOXj+qq7izMxs4FUd43+3pCkUc+28Hzg6It6ZlqfUWJ+ZmQ2wqp/q+RfgYuDsiHil0RgRz0n6Si2VmZlZLaoG/4eBVyLiDQBJGwHDIuLliLiiturMzGzAVR3jvwXYrLS+eWozM7NBpmrwD4uIlY2VtLx5PSWZmVmdqgb/S5LGNVYkvRd4pZfLm5nZeqrqGP+pwHWSngME/Cnw0bqKMjOz+lQK/oi4T9KeQGOmzcci4vX6yjIzs7r0Z5K2/YDRaZtxkoiIy2upyszMalMp+CVdAewKzAbeSM0BOPjNzAaZqkf844Gx6acSzcxsEKv6qZ6HKd7QNTOzQa7qEf9I4BFJ9wKvNhoj4phaqjIzs9pUDf5z6yzCzMzap+rHOW+X9HZgt4i4RdLmwJB6SzMzszpUnZb5U8D1wIWpaQfgxzXVZGZmNar65u5ngYOA5fDHH2X5k7qKMjOz+lQN/lcj4rXGiqShFJ/jNzOzQaZq8N8u6Wxgs/Rbu9cBP6mvLDMzq0vV4J8MdAMPAf8L+BnF7++amdkgU/VTPW8C300nMzMbxKrO1bOQFmP6EbHLgFdkZma16s9cPQ3DgP8GbDvw5ZiZWd0qjfFHxAul07MR8U8UP8BuZmaDTNWhnnGl1Y0oXgH0Zy5/MzNbT1QN72+XllcBTwLHD3g1ZmZWu6qf6nlf3YWYmVl7VB3qOb238yPi/IEpx8zM6tafT/XsB9yU1o8G7gXm1VGUmZnVp2rw7wiMi4gVAJLOBf4zIj5WV2FmZlaPqlM2jAJeK62/ltrMzGyQqXrEfzlwr6QfpfVjgctqqcjMzGpV9VM9X5P0c+Dg1PTJiHigvrLMzKwuVYd6ADYHlkfEVOAZSWOqbCRpiKQHJP00rY+RNEPSfEnXSNpkLeo2M7O1VPWnF78KnAV8KTVtDHy/4j6+CMwtrZ8HTImIdwC/B06ueD1mZjYAqh7xfwQ4BngJICKeA7bsayNJO1LM6XNxWhfwforf74XifYJj+1WxmZmtk6rB/1pEBGlqZklbVNzun4AzgTfT+tuApRGxKq0/Q/HD7WuQdIqkmZJmdnd3V9ydmZn1pWrwXyvpQmCEpE8Bt9DHj7JIOgpYEhGz1qawiLgoIsZHxPiurq61uQozM2uhz0/1pOGZa4A9geXAHsA5ETGtj00PAo6RdCTFHP5bAVMpnjyGpqP+HYFn16F+MzPrpz6DPyJC0s8iYm+gr7Avb/cl0pvBkv4SOCMiTpJ0HXAccDUwCbhxLeo2M7O1VHWo535J+w3QPs8CTpc0n2LM/5IBul4zM6ug6jd3/wz4mKQnKT7ZI4oXA++usnFE/Ar4VVpeAOzf30LNzGxg9Br8knaOiN8Ch7epHjMzq1lfR/w/ppiV8ylJN0TEX7ehJjMzq1FfY/wqLe9SZyFmZtYefQV/9LBsZmaDVF9DPftIWk5x5L9ZWoa33tzdqtbqzMxswPUa/BExpF2FmJlZe/RnWmYzM9sAOPjNzDLj4Dczy4yD38wsMw5+M7PMOPjNzDLj4Dczy4yD38wsMw5+M7PMOPjNzDLj4Dczy4yD38wsMw5+M7PMOPjNzDLj4Dczy4yD38wsMw5+M7PMOPjNzDLj4Dczy4yD38wsMw5+M7PMOPjNzDLj4Dczy4yD38wsMw5+M7PMOPjNzDLj4Dczy4yD38wsMw5+M7PMOPjNzDLj4Dczy0xtwS9pJ0m3SXpE0hxJX0zt20qaJmle+rtNXTWYmdma6jziXwX8bUSMBSYAn5U0FpgM3BoRuwG3pnUzM2uT2oI/IhZFxP1peQUwF9gBmAhcli52GXBsXTWYmdma2jLGL2k0sC8wAxgVEYvSWc8Do3rY5hRJMyXN7O7ubkeZZmZZqD34JQ0HbgBOjYjl5fMiIoBotV1EXBQR4yNifFdXV91lmpllo9bgl7QxRehfGRE/TM2LJW2Xzt8OWFJnDWZmtro6P9Uj4BJgbkScXzrrJmBSWp4E3FhXDWZmtqahNV73QcDfAA9Jmp3azga+AVwr6WTgKeD4GmswM7MmtQV/RNwBqIezD61rv2Zm1jt/c9fMLDMOfjOzzDj4zcwy4+A3M8uMg9/MLDMOfjOzzDj4zcwy4+A3M8uMg9/MLDMOfjOzzDj4zcwy4+A3M8uMg9/MLDMOfjOzzDj4zcwy4+A3M8uMg9/MLDMOfjOzzDj4zcwy4+A3M8uMg9/MLDMOfjOzzDj4zcwy4+A3M8uMg9/MLDMOfjOzzDj4zcwy4+A3M8uMg9/MLDMOfjOzzDj4zcwy4+A3M8uMg9/MLDMOfjOzzDj4zcwy4+A3M8uMg9/MLDNDO7FTSUcAU4EhwMUR8Y1O1GFmVsWUaY93ZL+nfWD3Wq637Uf8koYA/wp8CBgLnChpbLvrMDPLVSeGevYH5kfEgoh4DbgamNiBOszMstSJoZ4dgKdL688Af9Z8IUmnAKek1ZWSHluHfY4EfrcO2/fb6dUu1va6KnJd1a2PNYHr6q/1sq7T172ut7dq7MgYfxURcRFw0UBcl6SZETF+IK5rILmu/lkf61ofawLX1V+51dWJoZ5ngZ1K6zumNjMza4NOBP99wG6SxkjaBDgBuKkDdZiZZantQz0RsUrS54CbKT7OeWlEzKl5twMyZFQD19U/62Nd62NN4Lr6K6u6FBF1XK+Zma2n/M1dM7PMOPjNzDKzQQe/pCMkPSZpvqTJbd73TpJuk/SIpDmSvpjaz5X0rKTZ6XRkaZsvpVofk3R4jbU9KemhtP+ZqW1bSdMkzUt/t0ntkvTPqa7fSBpXU017lPpktqTlkk7tRH9JulTSEkkPl9r63T+SJqXLz5M0qaa6/lHSo2nfP5I0IrWPlvRKqd/+vbTNe9P9Pz/Vrhrq6vf9NtD/rz3UdU2ppiclzU7t7eyvnrKhfY+xiNggTxRvHD8B7AJsAjwIjG3j/rcDxqXlLYHHKaaoOBc4o8Xlx6YaNwXGpNqH1FTbk8DIprZvApPT8mTgvLR8JPBzQMAEYEab7rvnKb580vb+Ag4BxgEPr23/ANsCC9LfbdLyNjXU9UFgaFo+r1TX6PLlmq7n3lSrUu0fqqGuft1vdfy/tqqr6fxvA+d0oL96yoa2PcY25CP+jk4NERGLIuL+tLwCmEvxreWeTASujohXI2IhMJ/iNrTLROCytHwZcGyp/fIo3AOMkLRdzbUcCjwREU/1cpna+isipgMvtthff/rncGBaRLwYEb8HpgFHDHRdEfHLiFiVVu+h+F5Mj1JtW0XEPVGkx+Wl2zJgdfWip/ttwP9fe6srHbUfD1zV23XU1F89ZUPbHmMbcvC3mhqit+CtjaTRwL7AjNT0ufSS7dLGyznaW28Av5Q0S8XUGACjImJRWn4eGNWBuhpOYPV/yE73F/S/fzrRb/+D4siwYYykByTdLung1LZDqqUddfXnfmt3fx0MLI6IeaW2tvdXUza07TG2IQf/ekHScOAG4NSIWA5cAOwKvAdYRPFys93+PCLGUcyQ+llJh5TPTEc2Hfmcr4ov9R0DXJea1of+Wk0n+6cnkr4MrAKuTE2LgJ0jYl+KqaN+IGmrNpa03t1vTU5k9YOLtvdXi2z4o7ofYxty8Hd8aghJG1PcsVdGxA8BImJxRLwREW8C3+Wt4Ym21RsRz6a/S4AfpRoWN4Zw0t8l7a4r+RBwf0QsTjV2vL+S/vZP2+qT9AngKOCkFBikoZQX0vIsivHz3VMN5eGgWupai/utnf01FPgr4JpSvW3tr1bZQBsfYxty8Hd0aog0hngJMDcizi+1l8fHPwI0PnFwE3CCpE0ljQF2o3hTaaDr2kLSlo1lijcHH077b3wqYBJwY6muj6dPFkwAlpVejtZhtSOxTvdXSX/752bgg5K2ScMcH0xtA0rFjxqdCRwTES+X2rtU/PYFknah6J8Fqbblkiakx+jHS7dlIOvq7/3Wzv/Xw4BHI+KPQzjt7K+esoF2PsbW5d3p9f1E8W744xTP3l9u877/nOKl2m+A2el0JHAF8FBqvwnYrrTNl1Otj7GOnxzopa5dKD4x8SAwp9EvwNuAW4F5wC3AtqldFD+c80Sqe3yNfbYF8AKwdamt7f1F8cSzCHidYtz05LXpH4ox9/np9Mma6ppPMc7beIz9e7rsX6f7dzZwP3B06XrGUwTxE8B3SN/gH+C6+n2/DfT/a6u6Uvv3gE83Xbad/dVTNrTtMeYpG8zMMrMhD/WYmVkLDn4zs8w4+M3MMuPgNzPLjIPfzCwzDn4btCRNkXRqaf1mSReX1r8t6fRetv97SYf1sY9zJZ3Ron2EpM/0sM1tapotVMVMoxf0sp9fSVrvfuzbNkwOfhvM7gQOBJC0ETASeFfp/AOBu3raOCLOiYhb1nLfI4CWwU/x+fETmtqa5x8y6xgHvw1mdwEHpOV3UXzJZkX6JuOmwDuB+1XMp357mpTu5tLX4r8n6bi0fKSKee1nqZj7/Kel/YxNR+QLJH0htX0D2FXF3O3/2FTX9cCH0zdQGxNxbQ/8WtIFkmaqmIf971rdKEkrS8vHSfpeWu6SdIOk+9LpoLXsN8tc239s3WygRMRzklZJ2pni6P5uitkJDwCWUXzLMYB/ASZGRLekjwJfo/jGIwCShgEXAodExEJJzUfmewLvo5g7/bE0ZDMZ2Csi3tOirhcl3Usx79CNFEf710ZESPpyOn8IcKukd0fEbyre5KnAlIi4I93mmyme3Mz6xcFvg91dFKF/IHA+RfAfSBH8dwJ7AHsB04opUhhC8TX+sj0p5mVZmNavAk4pnf+fEfEq8KqkJbw1XW5vGsM9jeA/ObUfr2Iq7KEUP8gxluKr+1UcRvHqo7G+laThEbGyl23M1uDgt8GuMc6/N8VQz9PA3wLLgf+gmOdkTkQc0OM19O3V0vIbVPu/uRGYouJn8jaPiFlpUrIzgP0i4vdpCGdYi23L86iUz98ImBARf+hX9WZNPMZvg91dFFMSvxjFNMAvUrzxekA67zGgS9IBUEyHK+ldTdfxGLBLGosH+GiF/a6gGPppKR2F3wZcyltv6m4FvAQskzSKYiiolcWS3pnesP5Iqf2XwOcbK5LeU6FOszU4+G2we4ji0zz3NLUti4jfRfEzfscB50l6kGImxAPLVxARr1B8QucXkmZRhPqy3nYaxdztd0p6uMWbuw1XAfukv0TEg8ADwKPADyherbQyGfgpxRNXeVjqC8B4Fb9q9Qjw6d5qNOuJZ+c0o/g1pIhYmeZK/1dgXkRM6XRdZnXwEb9Z4VOSZlPMyb41xad8zDZIPuI3M8uMj/jNzDLj4Dczy4yD38wsMw5+M7PMOPjNzDLzX6ckGrm1wT+wAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot a histogram of the weights for the ridge, lasso, and count regression models. Discuss\n",
        "how the weights differ**\n",
        "\n",
        "For Ridge and Lasso regression, the weight is quite similar. It means that the penalty term applied is not really significant to the model.\n",
        "\n",
        "Lasso regression gives significantly lower loss. It's important to note that Lasso is better at handling multicollinearity by performing feature selection and shrinking the cofficient towards zero and more effective in handling outliers. This might indicates our data has high sparsity or multicollinearity which resulting in Lasso performs better than Ridge.  However, since the weights are similar, it means that both Ridge and Lasso are able to capture relationships between the attributes and target varible."
      ],
      "metadata": {
        "id": "h8iRKY2fo7gS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discuss and compare the behaviors of the models. Are there certain periods (ranges of\n",
        "years) in which models perform better than others? Where are the largest errors across\n",
        "models. Did regularization help for some models but not others?**\n"
      ],
      "metadata": {
        "id": "AGiCpk5-u0Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate model L2\n",
        "periods = [(1920, 1940), (1940, 1960), (1960, 1980), (1980, 2000), (2000, 2020)]\n",
        "\n",
        "for period in periods:\n",
        "    train_mask = (train_y >= period[0]) & (train_y < period[1])\n",
        "    test_mask = (test_y >= period[1]) & (test_y < period[1]+20)\n",
        "    train_x_period, train_y_period = train_x[train_mask], train_y[train_mask]\n",
        "    test_x_period, test_y_period = test_x[test_mask], test_y[test_mask]\n",
        "\n",
        "    # Train the model using mini-batch gradient descent\n",
        "    weight, train_losses, test_losses = mini_batch_gradient_descent(train_x_period, train_y_period, test_x_period, test_y_period, num_epochs=100, learning_rate=0.01, weight_decay_factor=0.001, loss_type='L2', weight_decay_form='L2')\n",
        "    \n",
        "    # Evaluate the performance of the model on the test set for the period\n",
        "    y_pred = np.dot(test_x_period, weight)\n",
        "    mse = np.mean(np.square(test_y_period - y_pred))\n",
        "    print(\"Period:\", period, \"MSE:\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "7qZdHQWqvktT",
        "outputId": "01122edf-f388-42b5-c56f-2b6b4fb6c611"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-4b56fadad6a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Train the model using mini-batch gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini_batch_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay_form\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Evaluate the performance of the model on the test set for the period\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-c850454e92f8>\u001b[0m in \u001b[0;36mmini_batch_gradient_descent\u001b[0;34m(X_train, y_train, X_test, y_test, batch_size, num_epochs, learning_rate, weight_decay_factor, loss_type, weight_decay_form, momentum, momentum_factor)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mtest_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-c850454e92f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(X, w)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (336,91) and (90,) not aligned: 91 (dim 1) != 90 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:, 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo5wVyN-zdBe",
        "outputId": "7cc79ab1-e4f8-4d84-c90e-cb059089afe2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         2001\n",
              "1         2001\n",
              "2         2001\n",
              "3         2001\n",
              "4         2001\n",
              "          ... \n",
              "515340    2006\n",
              "515341    2006\n",
              "515342    2006\n",
              "515343    2006\n",
              "515344    2005\n",
              "Name: target, Length: 515345, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate L2\n",
        "y_train_pred = np.dot(train_x, w_ridge)\n",
        "train_rmse = np.sqrt(np.mean(np.square(train_y - y_train_pred)))\n",
        "y_test_pred = np.dot(test_x, w_ridge)\n",
        "test_rmse = np.sqrt(np.mean(np.square(test_y - y_test_pred)))\n",
        "print(\"Train RMSE:\", train_rmse)\n",
        "print(\"Test RMSE:\", test_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSWXN3PMvFAP",
        "outputId": "54ef64ad-d978-40a4-a367-9d82d67dfe75"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train RMSE: 9.552927711558576\n",
            "Test RMSE: 9.512634607284436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate L1\n",
        "y_train_pred = np.dot(train_x, w_lasso)\n",
        "train_rmse = np.sqrt(np.mean(np.square(train_y - y_train_pred)))\n",
        "y_test_pred = np.dot(test_x, w_lasso)\n",
        "test_rmse = np.sqrt(np.mean(np.square(test_y - y_test_pred)))\n",
        "print(\"Train RMSE:\", train_rmse)\n",
        "print(\"Test RMSE:\", test_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUi6LwWvqyY5",
        "outputId": "1b8f3d19-7c49-45a7-c601-efc17fcbd075"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train RMSE: 9.870099705457289\n",
            "Test RMSE: 9.791397438648032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From RMSE above, L1 Lasso Regression gives larger error compared to Ridge."
      ],
      "metadata": {
        "id": "lMsyKIIZwOXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 5 - Softmax Properties\n",
        "\n",
        "\n",
        "###1. Show that the softmax function is invariant to constant offsets to its input\n",
        "\n",
        "\\begin{align*}\n",
        "softmax(a+c1) &= \\frac{exp(a+c1)}{\\sum_{j=1}^{n}exp(a_j+c)} \\\\\n",
        "&= \\frac{exp(a)exp(c1)}{\\sum_{j=1}^{n}exp(a_j)exp(c)}\\\\\n",
        "&= \\frac{exp(a)}{\\sum_{j=1}^{n}exp(a_j)} \\times \\frac{exp(c1)}{\\sum_{j=1}^{n}exp(c)}{}\\\\\n",
        "&= softmax(a) \\times \\frac{exp(c1)}{n \\times exp(c)}{}\\\\\n",
        "&= softmax(a) \\times \\frac{exp(c1-c)}{n}\n",
        "\\end{align*}\n",
        "\n",
        "Since $\\frac{exp(c1-c)}{n}$ is a constant, it doesn't depend on $a$ and it's proven that softmax function is invariant to constant offset to its input.\n"
      ],
      "metadata": {
        "id": "Ri2DQfSOEA6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.  Why is the observation that the softmax function is invariant to constant offsets to its input important when implementing it in a neural network?\n",
        "\n",
        "It's important that softmax function is invariant to constant offset to its input because it allows the model to be more robust to changes in input data. Adding or substracting a constant value from input data doesn't change the output of sofmax function and making the network to generalize better to new data that can improve reliability and accuracy."
      ],
      "metadata": {
        "id": "Pl2YQkyDNU7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 6 - Implementing Softmax Classifier\n",
        "\n",
        "Write a function to load the data and the labels, which are returned as NumPy arrays."
      ],
      "metadata": {
        "id": "d7XFmHfPP_t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pXxYofx33qwQ",
        "outputId": "5a34087b-c7a2-46a1-bc16-09164de83ab6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_iris_data(train_file, test_file):\n",
        "    train_data = np.loadtxt(train_file)\n",
        "    test_data = np.loadtxt(test_file)\n",
        "    train_labels = train_data[:, 0].astype(int)\n",
        "    train_features = train_data[:, 1:]\n",
        "    test_labels = test_data[:, 0].astype(int)\n",
        "    test_features = test_data[:, 1:]\n",
        "    return train_features, train_labels, test_features, test_labels\n",
        "\n",
        "iris_train = '/content/drive/MyDrive/Deep Learning/HW2/iris-train.txt'\n",
        "iris_test = '/content/drive/MyDrive/Deep Learning/HW2/iris-test.txt'\n",
        "\n",
        "train_features, train_labels, test_features, test_labels = load_iris_data(iris_train, iris_test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_features, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "XxrvhNu63P7s",
        "outputId": "e29bbe7c-252d-4851-a425-a99bf482d78a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(72, 2)\n",
            "(18, 2)\n",
            "(72,)\n",
            "(18,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Implementation & Evaluation\n",
        "\n",
        "* Use softmax loss with L2 weight decay regularization  \n",
        "*  Use stochastic gradient descent with mini batches and momentum to minimize softmax loss of single LNN (use environment's BLAS)\n",
        "* Loop over epochs and mini batches (not individual vector/ matrices)\n",
        "* 1000 epochs\n",
        "* Normalize feature between -1 and 1\n",
        "* Initial weight from Gaussian distribution\n",
        "\n",
        "*Cross entropy loss*\n",
        "\n",
        "$y_{i,j}$: true label for the $i$-th sample and $j$-th class\n",
        "\n",
        "$p_{i,j}$: predicted probability for the $i$-th sample and $j$-th class. \n",
        "$$ L = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^C y_{i,j} \\log(p_{i,j}) $$\n",
        "\n",
        "\n",
        "What is the best test accuracy your model achieved? What hyperparameters did you use?\n",
        "Would early stopping have helped improve accuracy on the test data?\n",
        "\n"
      ],
      "metadata": {
        "id": "TG4ZLs-d3nDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize feature to be between -1 and 1\n",
        "X_train= (X_train - np.mean(X_train, axis=0)) / (np.std(X_train, axis=0)+ 1e-8)\n",
        "X_test= (X_test - np.mean(X_test, axis=0)) / (np.std(X_test, axis=0)+1e-8)\n",
        "\n",
        "#Add bias to the features\n",
        "X_train= np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
        "X_test= np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    p_x = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "    return p_x\n",
        "\n",
        "def cross_entropy_loss(y, y_pred, w, weight_decay_factor):\n",
        "    num_examples = y.shape[0]\n",
        "    loss = -np.sum(np.log(y_pred[np.arange(num_examples), y]))\n",
        "    loss += (weight_decay_factor / 2) * np.sum(np.square(w))\n",
        "    return loss / num_examples\n",
        "\n",
        "def mean_per_class_accuracy(y, y_pred):\n",
        "    num_classes = np.unique(y).shape[0]\n",
        "    accuracy = 0\n",
        "    for c in range(num_classes):\n",
        "        mask = (y == c)\n",
        "        if np.sum(mask) > 0:\n",
        "            accuracy += np.sum(y_pred[mask, c]) / np.sum(mask)\n",
        "    return accuracy / num_classes\n",
        "\n",
        "def mini_batch_gradient_descent(X_train, y_train, X_test, y_test, num_epochs=1000, batch_size=32, learning_rate=0.01, momentum_rate=0.9, weight_decay_factor=0.01):\n",
        "    num_examples, num_features = X_train.shape\n",
        "    num_batches = int(np.ceil(num_examples / batch_size))\n",
        "    w = np.random.randn(num_features, np.unique(y_train).shape[0]) * 0.01 #initial weight with gaussian distribution\n",
        "    v = np.zeros((num_features, np.unique(y_train).shape[0]))\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = min((i + 1) * batch_size, num_examples)\n",
        "            X_batch = X_train[start_idx:end_idx]\n",
        "            y_batch = y_train[start_idx:end_idx]\n",
        "            y_pred = softmax(np.dot(X_batch, w))\n",
        "            grad = np.dot(X_batch.T, y_pred - np.eye(np.unique(y_train).shape[0])[y_batch.astype(int)]) / X_batch.shape[0]\n",
        "            v = momentum_rate * v - learning_rate * grad\n",
        "            w += v\n",
        "        # Compute the training loss and accuracy\n",
        "        y_pred_train = softmax(np.dot(X_train, w))\n",
        "        train_loss = cross_entropy_loss(y_train, y_pred_train, w, weight_decay_factor)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracy = mean_per_class_accuracy(y_train, y_pred_train)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        # Compute the testing loss and accuracy\n",
        "        y_pred_test = softmax(np.dot(X_test, w))\n",
        "        test_loss = cross_entropy_loss(y_test, y_pred_test, w, weight_decay_factor)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracy = mean_per_class_accuracy(y_test, y_pred_test)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "    return w, train_losses, train_accuracies, test_losses, test_accuracies\n"
      ],
      "metadata": {
        "id": "501xlYqs2m77"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight, train_losses, train_accuracies, test_losses, test_accuracies= mini_batch_gradient_descent(X_train, y_train, X_test, y_test, num_epochs=1000, batch_size=32, learning_rate=0.01, momentum_rate=0.9, weight_decay_factor=0.01)"
      ],
      "metadata": {
        "id": "CsrvBqRB_rpZ",
        "outputId": "dc2dad8d-2bce-42df-f62a-89209c926485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-6d3b8e82810a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracies\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmini_batch_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-8475beac989e>\u001b[0m in \u001b[0;36mmini_batch_gradient_descent\u001b[0;34m(X_train, y_train, X_test, y_test, num_epochs, batch_size, learning_rate, momentum_rate, weight_decay_factor)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmomentum_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plot the cross-entropy loss during training for both the train and test sets\n",
        "ax1.plot(range(num_epochs), train_losses, label='Train')\n",
        "ax1.plot(range(num_epochs), test_losses, label='Test')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Cross-entropy loss')\n",
        "ax1.set_title('Training and test loss')\n",
        "ax1.legend()\n",
        "\n",
        "# Plot the accuracy during training for both the train and test sets\n",
        "ax2.plot(range(num_epochs), train_accuracies, label='Train')\n",
        "ax2.plot(range(num_epochs), test_accuracies, label='Test')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Mean per-class accuracy')\n",
        "ax2.set_title('Training and test accuracy')\n",
        "ax2.legend()\n",
        "\n",
        "# Show the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JmxJUPiQ__DC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}